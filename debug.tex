\section{Working with Boolean
Functions}\label{working-with-boolean-functions}

Boolean functions are the building blocks of Boolean network models used
to represent gene regulatory networks, signaling pathways, and other
biological control systems. Understanding how to create and analyze
individual Boolean functions is essential before studying network-level
dynamics.

In this tutorial, we explore the \texttt{BooleanFunction} class --- the
foundation of BoolForge. Boolean functions form the regulatory rules in
Boolean network models of gene regulation, so understanding their
structure is essential before studying networks.

\subsection{What you will learn}\label{what-you-will-learn}

In this tutorial you will:

\begin{itemize}
\tightlist
\item
  create Boolean functions from truth tables and from textual
  expressions,
\item
  inspect core attributes such as degree, variable names, and stored
  properties,
\item
  compute basic structural properties (essential variables, Hamming
  weight, bias),
\item
  convert Boolean functions into logical and polynomial representations,
\item
  and interface with CANA objects.
\end{itemize}

\subsection{Setup}\label{setup}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\end{Highlighting}
\end{Shaded}

\subsection{Create a Boolean function}\label{create-a-boolean-function}

Boolean functions can be described in logical form, as polynomials, or
as truth tables. BoolForge treats Boolean functions as binary vectors of
length \(2^n\), where \(n\) is the number of inputs. The vectors
describe the \emph{right side} of the truth table. The left side of the
truth table is not stored because it is the same for any function with n
inputs. For example, the function \[
f(A,B) = A \land B
\] is stored as \texttt{{[}0,\ 0,\ 0,\ 1{]}}, corresponding to:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
A & B & f(A,B) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 0 \\
1 & 1 & 1 \\
\end{longtable}
}

\subsubsection{Create Boolean functions from a truth
table}\label{create-boolean-functions-from-a-truth-table}

An instance of \texttt{BooleanFunction} can be generated by specifying
the right side of the truth table, i.e., by providing a binary vector of
length \(2^n\) for any \(n\geq 0\). For example, to create the AND
function above, we can write

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], name}\OperatorTok{=}\StringTok{"f\_AND"}\NormalTok{) }\CommentTok{\#name is optional}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f:"}\NormalTok{, f)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Truth table of f:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, f.to\_truth\_table().to\_string())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
f: [0 0 0 1]
Truth table of f:
    x0  x1  f_AND
0   0   0      0
1   0   1      0
2   1   0      0
3   1   1      1
\end{verbatim}

Any Boolean function is stored as right side of the truth table. That
is, the outputs are ordered by the binary representation of inputs:

\begin{itemize}
\tightlist
\item
  Position 0 → (A,B) = (0,0)
\item
  Position 1 → (A,B) = (0,1)\\
\item
  Position 2 → (A,B) = (1,0)
\item
  Position 3 → (A,B) = (1,1)
\end{itemize}

\subsubsection{Create Boolean functions from
text}\label{create-boolean-functions-from-text}

Boolean functions can also be created from textual expressions. For
example, to define the same function as f, we can write

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f2 }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"A and B"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f2:"}\NormalTok{, f2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
f2: [0 0 0 1]
\end{verbatim}

The text processor is fairly versatile. For example, we can define the
same function as f also by writing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f3 }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"A + B \textgreater{} 1"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f3:"}\NormalTok{, f3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
f3: [0 0 0 0]
\end{verbatim}

Some examples of more complicated functions include:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"(A AND B) OR (NOT A AND C)"}\NormalTok{)}
\NormalTok{h }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"(x + y + z) \% 2 == 0"}\NormalTok{)}
\NormalTok{k }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"({-}1) * x + y + z \textgreater{} 0"}\NormalTok{)}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"g"}\NormalTok{, }\StringTok{"h"}\NormalTok{, }\StringTok{"k"}\NormalTok{]}
\NormalTok{boolforge.display\_truth\_table(g, h, k, labels}\OperatorTok{=}\NormalTok{labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x1  x2  x3  |   g   h   k
-------------------------------------------------
0   0   0   |   0   1   0
0   0   1   |   1   0   1
0   1   0   |   0   0   1
0   1   1   |   1   0   1
1   0   0   |   0   0   0
1   0   1   |   0   0   0
1   1   0   |   1   0   0
1   1   1   |   1   0   1
\end{verbatim}

\subsubsection{Combining BooleanFunction
objects}\label{combining-booleanfunction-objects}

New Boolean functions can be constructed by combining existing ones
using Boolean algebra operations. This is useful when building larger
rules from simpler components.

Supported operations include:

\begin{itemize}
\tightlist
\item
  \texttt{\textasciitilde{}} NOT
\item
  \texttt{\&} AND
\item
  \texttt{\textbar{}} OR
\item
  \texttt{\^{}} XOR
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"X + Y == 1"}\NormalTok{)}
\NormalTok{b }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"X OR Y"}\NormalTok{)}

\NormalTok{not\_a }\OperatorTok{=} \OperatorTok{\textasciitilde{}}\NormalTok{a}
\NormalTok{a\_and\_b }\OperatorTok{=}\NormalTok{ a }\OperatorTok{\&}\NormalTok{ b}
\NormalTok{a\_or\_b }\OperatorTok{=}\NormalTok{ a }\OperatorTok{|}\NormalTok{ b}
\NormalTok{a\_xor\_b }\OperatorTok{=}\NormalTok{ a }\OperatorTok{\^{}}\NormalTok{ b}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"NOT a"}\NormalTok{, }\StringTok{"a AND b"}\NormalTok{, }\StringTok{"a OR b"}\NormalTok{, }\StringTok{"a XOR b"}\NormalTok{]}
\NormalTok{boolforge.display\_truth\_table(a, b, not\_a, a\_and\_b, a\_or\_b, a\_xor\_b, labels}\OperatorTok{=}\NormalTok{labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x1  x2  |   a   b   NOT a   a AND b a OR b  a XOR b
-----------------------------------------------------------------------
0   0   |   0   0   1   0   0   0
0   1   |   1   1   0   1   1   0
1   0   |   1   1   0   1   1   0
1   1   |   1   1   0   1   1   0
\end{verbatim}

\subsection{Attributes of
BooleanFunction}\label{attributes-of-booleanfunction}

Each \texttt{BooleanFunction} instance has the following attributes:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
attribute & type & description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{f} & \texttt{np.ndarray} & truth table (right side) \\
\texttt{n} & \texttt{int} & number of variables \\
\texttt{variables} & \texttt{np.ndarray} & variable names \\
\texttt{name} & \texttt{str} & optional name \\
\texttt{properties} & \texttt{dict} & cached properties \\
\end{longtable}
}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"f.f:"}\NormalTok{, f.f)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f.n:"}\NormalTok{, f.n)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f.variables:"}\NormalTok{, f.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f.name:"}\NormalTok{, f.name)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f.properties:"}\NormalTok{, f.properties)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
f.f: [0 0 0 1]
f.n: 2
f.variables: ['x0' 'x1']
f.name: f_AND
f.properties: {}
\end{verbatim}

When a function is created from a truth table, variable names default to
\texttt{x0,\ x1,\ ...}. When created from text, variable names are
inferred.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"f2.variables:"}\NormalTok{, f2.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f3.variables:"}\NormalTok{, f3.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"g.variables:"}\NormalTok{, g.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"h.variables:"}\NormalTok{, h.variables)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
f2.variables: ['A' 'B']
f3.variables: ['A' 'B']
g.variables: ['A' 'B' 'C']
h.variables: ['x' 'y' 'z']
\end{verbatim}

The variable order is determined by first occurrence in the expression.
See e.g.,

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(boolforge.BooleanFunction(}\StringTok{"(x + y + z) \% 2 == 0"}\NormalTok{).variables)}
\BuiltInTok{print}\NormalTok{(boolforge.BooleanFunction(}\StringTok{"(y + z + x) \% 2 == 0"}\NormalTok{).variables)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['x' 'y' 'z']
['y' 'z' 'x']
\end{verbatim}

The variable order determines how the truth table is indexed. For
example, for variables {[}x,y,z{]}, the entry in position i corresponds
to the binary expansion of i over (x,y,z). Therefore, the same
expression with a different variable order results in a different
right-side truth table ordering. This becomes important when combining
functions inside networks or importing networks from text files.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Basic properties of Boolean
functions}\label{basic-properties-of-boolean-functions}

We can inspect various properties of a Boolean function. The degree,
i.e., the number of inputs, is readily available via `f.n'. Other
properties can be computed.

\begin{itemize}
\tightlist
\item
  `.is\_constant()' checks if the function is constant,
\item
  `.is\_degenerate()' checks if the function contains non-essential
  variables,
\item
  `.get\_essential\_variables()' provides the indices (Python: starting
  at 0!) of the essential variables,
\item
  `.get\_type\_of\_inputs()' describes the type of each input
  (`positive', `negative', `conditional', or `non-essential').
\item
  The Hamming weight is the number of 1s in the right side of the truth
  table.
\item
  The absolute bias is \(|\text{\#ones} - \text{\#zeros}| / 2^n\). It
  equals 1 for constant functions and 0 for unbiased functions.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Number of variables:"}\NormalTok{, f.n)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Is constant?"}\NormalTok{, f.is\_constant())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Is degenerate?"}\NormalTok{, f.is\_degenerate())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Essential variables:"}\NormalTok{, f.get\_essential\_variables())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Type of inputs:"}\NormalTok{, f.get\_type\_of\_inputs())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hamming weight:"}\NormalTok{, f.get\_hamming\_weight())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Absolute bias:"}\NormalTok{, f.get\_absolute\_bias())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of variables: 2
Is constant? False


Is degenerate? False
Essential variables: [0, 1]
Type of inputs: ['positive' 'positive']
Hamming weight: 1
Absolute bias: 0.5
\end{verbatim}

Repeating this for \texttt{g} illustrates how properties differ.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Number of variables:"}\NormalTok{, g.n)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Is constant?"}\NormalTok{, g.is\_constant())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Is degenerate?"}\NormalTok{, g.is\_degenerate())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Essential variables:"}\NormalTok{, g.get\_essential\_variables())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Type of inputs:"}\NormalTok{, g.get\_type\_of\_inputs())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hamming weight:"}\NormalTok{, g.get\_hamming\_weight())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Absolute bias:"}\NormalTok{, g.get\_absolute\_bias())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of variables: 3
Is constant? False
Is degenerate? False
Essential variables: [0, 1, 2]
Type of inputs: ['positive' 'positive' 'conditional']
Hamming weight: 4
Absolute bias: 0.0
\end{verbatim}

The \texttt{.summary()} method prints a human-readable overview of basic
properties. If more advanced properties have already been computed,
e.g., by \texttt{.get\_layer\_structure()} or
\texttt{get\_type\_of\_inputs()}, they are also displayed (or if the
optional keyword \texttt{COMPUTE\_ALL} is set to True, default False).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"(A and B) OR NOT C"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(f.summary())}
\BuiltInTok{print}\NormalTok{()}

\CommentTok{\# Trigger computation of more advanced properties}
\BuiltInTok{print}\NormalTok{(f.summary(compute\_all}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
BooleanFunction summary
----------------------------------------
Number of variables:        3
Hamming Weight:             5
Bias:                       0.625
Absolute bias:              0.250
Variables:                  ['A' 'B' 'C']

BooleanFunction summary
----------------------------------------
Number of variables:        3
Hamming Weight:             5
Bias:                       0.625
Absolute bias:              0.250
Variables:                  ['A' 'B' 'C']
CanalizingDepth:            3
NumberOfLayers:             2
CanalizingInputs:           [0 0 0]
CanalizedOutputs:           [1 0 0]
CoreFunction:               [1]
OrderOfCanalizingVariables: [2 0 1]
LayerStructure:             [1, 2]
InputTypes:                 ['negative' 'positive' 'positive']
\end{verbatim}

\subsection{Logical and polynomial
representations}\label{logical-and-polynomial-representations}

While Boolean functions are stored as truth tables, they can be
expressed in logical and polynomial format.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Logical form of }\SpecialCharTok{\{}\NormalTok{f}\SpecialCharTok{.}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{, f.to\_logical(and\_op}\OperatorTok{=}\StringTok{" ∧ "}\NormalTok{, or\_op}\OperatorTok{=}\StringTok{" ∨ "}\NormalTok{, not\_op}\OperatorTok{=}\StringTok{" ¬"}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Polynomial form of }\SpecialCharTok{\{}\NormalTok{f}\SpecialCharTok{.}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{, f.to\_polynomial())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Logical form of : (( ¬C)) ∨ (A ∧ B)
Polynomial form of : (1 - A) * (1 - B) * (1 - C) + (1 - A) * B * (1 - C) + A * (1 - B) * (1 - C) + A * B * (1 - C) + A * B * C
\end{verbatim}

In addition, an instance of \texttt{BooleanFunction} can be turned into
an instance of \texttt{BooleanNode} from the
\href{https:www.github.com}{CANA package}. This requires the optional
CANA package to be installed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cana\_object }\OperatorTok{=}\NormalTok{ f.to\_cana()}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{type}\NormalTok{(cana\_object))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<class 'cana.boolean_node.BooleanNode'>
\end{verbatim}

\subsection{Summary of Key Concepts}\label{summary-of-key-concepts}

Before moving on to more advanced topics, here is a short summary of the
fundamental ideas introduced in this tutorial:

\subsubsection{\texorpdfstring{\textbf{Boolean
functions}}{Boolean functions}}\label{boolean-functions}

A Boolean function maps a set of binary inputs (0/1) to a single binary
output. BoolForge represents Boolean functions internally by their truth
table, i.e., the list of outputs in lexicographic order of the input
combinations.

\subsubsection{\texorpdfstring{\textbf{Representations of Boolean
functions}}{Representations of Boolean functions}}\label{representations-of-boolean-functions}

Boolean functions can be created from:

\begin{itemize}
\tightlist
\item
  a truth table (list of 0s and 1s),
\item
  a logical expression written in Python syntax,
\item
  algebraic combinations of existing BooleanFunction objects using
  operations such as\\
  \texttt{+} (OR), \texttt{*} (AND), \texttt{\^{}} (XOR), and other
  supported Boolean operations.
\end{itemize}

Each representation produces an equivalent internal truth-table-based
object.

\subsubsection{\texorpdfstring{\textbf{Variable names and
ordering}}{Variable names and ordering}}\label{variable-names-and-ordering}

BoolForge automatically infers variable names from the order of first
appearance in expressions.\\
This order determines the indexing of the truth table and therefore
affects how the function interacts with larger Boolean networks.

\subsubsection{\texorpdfstring{\textbf{Basic properties of Boolean
functions}}{Basic properties of Boolean functions}}\label{basic-properties-of-boolean-functions-1}

BoolForge can compute structural properties, including:

\begin{itemize}
\tightlist
\item
  the number of variables (\texttt{n}),
\item
  the Hamming weight (number of 1s in the truth table),
\item
  absolute bias (imbalance between 0s and 1s),
\item
  essential and non-essential variables,
\item
  positive/negative influence of each input.
\end{itemize}

These properties help characterize the function's behavior and are used
throughout later tutorials.

\subsubsection{\texorpdfstring{\textbf{Conversions and
interoperability}}{Conversions and interoperability}}\label{conversions-and-interoperability}

BoolForge supports conversion between representations (e.g., truth table
to/from polynomial form) and is compatible with external packages such
as \textbf{CANA} for advanced analysis.\\
This makes it easy to move between analytical frameworks and reuse
models.

Together, these concepts provide the foundation for understanding
canalization, random Boolean function generation, and eventually the
construction and analysis of full Boolean networks.

\section{Frequently Asked Questions
(FAQ)}\label{frequently-asked-questions-faq}

\subsection{Why does the order of variables
matter?}\label{why-does-the-order-of-variables-matter}

The order in which variables appear determines the ordering of the truth
table. For a function with variables \texttt{{[}A,\ B,\ C{]}}, the entry
at position \texttt{i} corresponds to the binary representation of
\texttt{i} over \texttt{(A,\ B,\ C)}.

If two equivalent expressions list variables in different orders, their
truth tables will be indexed differently.

To ensure reproducibility, always use consistent variable names and
ordering.

\subsection{How do I choose between defining a function via a truth
table or via an
expression?}\label{how-do-i-choose-between-defining-a-function-via-a-truth-table-or-via-an-expression}

Short answer: It does not matter. Both methods produce identical
internal representations.

Slightly longer answer: Use a \textbf{textual expression} if:

\begin{itemize}
\tightlist
\item
  you know the natural logical description of your function (e.g.,
  \texttt{A\ and\ B}),
\item
  the function is part of a Boolean network stored in some text file.
\end{itemize}

Use a \textbf{truth table} if:

\begin{itemize}
\tightlist
\item
  you generated the table programmatically (e.g., using
  \texttt{boolforge.random\_function}).
\end{itemize}

\subsection{\texorpdfstring{What is the difference between
\texttt{get\_type\_of\_inputs()} and
monotonicity?}{What is the difference between get\_type\_of\_inputs() and monotonicity?}}\label{what-is-the-difference-between-get_type_of_inputs-and-monotonicity}

The method \texttt{get\_type\_of\_inputs()} classifies each input
variable individually according to how it influences the output:

\begin{itemize}
\tightlist
\item
  positively increasing,
\item
  negatively increasing,
\item
  conditional,
\item
  or non-essential.
\end{itemize}

Monotonicity, by contrast, is a \textbf{global property} of the Boolean
function. A function is monotone if \textbf{all} essential variables
influence the output in a consistent direction.

A function can therefore be non-monotone even if individual inputs have
a well-defined influence type.

\subsection{Quick Reference}\label{quick-reference}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Task & Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Create from truth table &
\texttt{BooleanFunction({[}0,\ 0,\ 0,\ 1{]})} \\
Create from expression & \texttt{BooleanFunction("A\ and\ B")} \\
Combine with operations &
\texttt{f\ \&\ g,\ f\ \textbackslash{}\textbar{}\ g,\ \textasciitilde{}f,\ f\ \^{}\ g} \\
Check properties & \texttt{f.is\_constant()},
\texttt{f.is\_degenerate()} \\
Get variable info & \texttt{f.variables}, \texttt{f.n} \\
Convert representations & \texttt{f.to\_logical()},
\texttt{f.to\_polynomial()} \\
\end{longtable}
}

\section{Advanced Concepts for Boolean
Functions}\label{advanced-concepts-for-boolean-functions}

Understanding the structure of a Boolean function is essential for
analyzing the behavior of the Boolean networks they define. In this
tutorial, we move beyond the basics of \texttt{BooleanFunction} and
explore three core concepts:

\begin{itemize}
\tightlist
\item
  \textbf{Symmetries} among inputs
\item
  \textbf{Activities} of inputs
\item
  \textbf{Average sensitivity} of a Boolean function
\end{itemize}

These quantities are tied to redundancy, robustness, and dynamical
behavior -- concepts that will play a central role in later tutorials on
canalization and network dynamics.

\subsection{What you will learn}\label{what-you-will-learn-1}

In this tutorial you will learn how to:

\begin{itemize}
\tightlist
\item
  identify symmetry groups of Boolean functions,
\item
  compute activities and sensitivities,
\item
  choose between exact and Monte Carlo computation,
\item
  interpret these quantities in terms of robustness and redundancy.
\end{itemize}

\subsection{Setup}\label{setup-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\end{Highlighting}
\end{Shaded}

\subsection{Symmetries in Boolean
Functions}\label{symmetries-in-boolean-functions}

In gene regulation, symmetric variables might represent redundant
transcription factor binding sites or functionally equivalent
repressors. Identifying symmetries can: - Reduce model complexity -
Suggest evolutionary mechanisms (gene duplication) - Identify potential
drug targets (symmetric inputs may compensate)

\subsubsection{What is a symmetry?}\label{what-is-a-symmetry}

A symmetry of a Boolean function is a permutation of input variables
that does \textbf{not} change its output.

\begin{itemize}
\tightlist
\item
  Inputs in the same symmetry group can be swapped freely.
\item
  Inputs in different groups cannot.
\end{itemize}

The following three Boolean functions exhibit full, partial, and no
symmetry.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fully symmetric (parity / XOR)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"(x0 + x1 + x2) \% 2"}\NormalTok{)}

\CommentTok{\# Partially symmetric}
\NormalTok{g }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"x0 | (x1 \& x2)"}\NormalTok{)}

\CommentTok{\# No symmetry}
\NormalTok{h }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"x0 | (x1 \& \textasciitilde{}x2)"}\NormalTok{)}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"f"}\NormalTok{, }\StringTok{"g"}\NormalTok{, }\StringTok{"h"}\NormalTok{]}
\NormalTok{boolforge.display\_truth\_table(f, g, h, labels}\OperatorTok{=}\NormalTok{labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0  x1  x2  |   f   g   h
-------------------------------------------------
0   0   0   |   0   0   0
0   0   1   |   1   0   0
0   1   0   |   1   0   1
0   1   1   |   1   1   0
1   0   0   |   1   1   1
1   0   1   |   1   1   1
1   1   0   |   1   1   1
1   1   1   |   1   1   1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h], labels):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Symmetry groups of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ group }\KeywordTok{in}\NormalTok{ func.get\_symmetry\_groups():}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"  "}\NormalTok{, func.variables[np.array(group)])}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Symmetry groups of f:
   ['x0' 'x1' 'x2']

Symmetry groups of g:
   ['x0']
   ['x1' 'x2']

Symmetry groups of h:
   ['x0']
   ['x1']
   ['x2']
\end{verbatim}

\textbf{Interpretation}

\begin{itemize}
\tightlist
\item
  \texttt{f} is fully symmetric: all variables are interchangeable.
\item
  \texttt{g} has partial symmetry: \texttt{x1} and \texttt{x2} are
  equivalent but \texttt{x0} is distinct.
\item
  \texttt{h} has no symmetries: all inputs play unique roles.
\end{itemize}

These patterns foreshadow the concepts of canalization, and specifically
canalizing layers, explored in later tutorials.

\subsection{Degenerate functions}\label{degenerate-functions}

A function is \textbf{degenerate} if one or more inputs do not matter at
all.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"f.is\_degenerate()"}\NormalTok{, f.is\_degenerate())}
\NormalTok{k }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"(x AND y) OR x"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"k.is\_degenerate()"}\NormalTok{, k.is\_degenerate())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
f.is_degenerate() False
k.is_degenerate() True
\end{verbatim}

Detecting degeneracy is NP-hard in general. However, such functions are
extremely rare unless intentionally created.

BoolForge therefore:

\begin{itemize}
\tightlist
\item
  allows degenerate functions by default,
\item
  avoids expensive essential-variable checks unless requested.
\end{itemize}

\subsection{Activities and
Sensitivities}\label{activities-and-sensitivities}

Activities and sensitivity quantify how much each input affects the
output of a Boolean function.

\subsubsection{Activity}\label{activity}

The activity of input \(x_i\) is the probability that flipping \(x_i\)
changes the function's output:

\[
a(f,x_i) = \Pr[f(\mathbf{x}) \neq f(\mathbf{x} \oplus e_i)],
\] where \(e_i=(0,\ldots,0,1,0,\ldots,0)\) is the ith unit vector.

\begin{itemize}
\tightlist
\item
  If \(a = 1\): the variable always matters.
\item
  If \(a = 0\): the variable is irrelevant (degenerate).
\item
  In large random Boolean functions, \(a \approx 0.5\) for all
  variables.
\end{itemize}

\subsubsection{Average sensitivity}\label{average-sensitivity}

The \emph{average sensitivity} of a Boolean function describes how
sensitive its output is to changes in its inputs, specifically to a
random single-bit flip. The (unnormalized) average sensitivity is the
sum of all its activities:

\[
S(f) = \sum_i a(f,x_i).
\]

Division by \(n\) yields the \emph{normalized average sensitivity}
\(s(f)\), which can be readily compared between functions of different
degree \(n\):

\[
s(f) = \frac{S(f)}{n}.
\]

\textbf{Interpretation}

In Boolean network theory, the mean normalized average sensitivity
\(s(f)\) determines how perturbations tend to propagate through the
system.

\begin{itemize}
\tightlist
\item
  If \(s(f) < 1\), perturbations tend to die out (\emph{ordered
  regime}).
\item
  If \(s(f) > 1\), perturbations typically amplify (\emph{chaotic
  regime}).
\item
  The boundary \(s(f) = 1\) defines the \emph{critical regime}.
\end{itemize}

The critical regime is believed to characterize many biological networks
(see later tutorials). It represents a balance between order and chaos.
Operating at this ``edge of chaos'' may optimize information processing
and evolvability.

\subsubsection{Exact vs Monte Carlo
computation}\label{exact-vs-monte-carlo-computation}

\begin{itemize}
\tightlist
\item
  Exact (\texttt{exact=True}) computation enumerates all \(2^n\) states;
  feasible for small \(n\).
\item
  Monte Carlo (\texttt{exact=False}, default) simulation approximates
  using random samples; scalable to large \(n\).
\end{itemize}

Computational cost guide: - Exact methods: \(O(2^n)\) time and space,
where \(n =\) number of inputs. - Monte Carlo: \(O(k)\) time, where
\(k =\) number of samples.

Recommendation: - \(n \leq 10\): Use exact methods (fast, deterministic)
- \(10 < n \leq 20\): Use exact if possible, Monte Carlo if repeated
computation needed - n \textgreater{} 20: Use Monte Carlo (exact is
infeasible)

\subsubsection{Computing activities and
sensitivities}\label{computing-activities-and-sensitivities}

To investigate how to compute the activities and the average sensitivity
in \texttt{BoolForge}, we work with the linear function \texttt{f} from
above, as well as with the function \texttt{g}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exact }\OperatorTok{=} \VariableTok{True}
\NormalTok{normalized }\OperatorTok{=} \VariableTok{True}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Activities of f:"}\NormalTok{, f.get\_activities(exact}\OperatorTok{=}\NormalTok{exact))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Activities of g:"}\NormalTok{, g.get\_activities(exact}\OperatorTok{=}\NormalTok{exact))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Normalized average sensitivity of f:"}\NormalTok{, f.get\_average\_sensitivity(exact}\OperatorTok{=}\NormalTok{exact, normalized}\OperatorTok{=}\NormalTok{normalized))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normalized average sensitivity of g:"}\NormalTok{, g.get\_average\_sensitivity(exact}\OperatorTok{=}\NormalTok{exact, normalized}\OperatorTok{=}\NormalTok{normalized))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Activities of f: [0.25 0.25 0.25]
Activities of g: [0.75 0.25 0.25]
Normalized average sensitivity of f: 0.25
Normalized average sensitivity of g: 0.4166666666666667
\end{verbatim}

\textbf{Interpretation}

\begin{itemize}
\tightlist
\item
  For \texttt{f} (XOR), flipping any input always flips the output, so
  \(s(f) = 1\).
\item
  For \texttt{g}, \(x_0\) influences the output more often than \(x_1\)
  or \(x_2\). 75\% of \(x_0\) flips and 25\% of \(x_1\) or \(x_2\) flips
  change the output of \texttt{g}. Thus, the normalized average
  sensitivity of \texttt{g} is
  \(\frac 13*75\% + \frac 23 25\% = \frac{5}{12}\).
\end{itemize}

This unequal influence is a precursor to canalization, a property
investigated in depth in the next tutorial.

Exact computation is infeasible for large \(n\), so Monte Carlo
simulation must be used.

When generating such a large function randomly (see Tutorial 4) it not
recommended to require that all inputs are essential, as (i) this is
almost certainly the case anyways (the probability that an n-input
function does not depend on input \(x_i\) is given \(1/2^{n-1}\)), and
(ii) checking for input degeneracy is NP-hard (i.e., very
computationally expensive). We thus set
\texttt{allow\_degenerate\_functions=True}. You find more on this and
the \texttt{random\_function} method in Tutorial 4.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exact }\OperatorTok{=} \VariableTok{False}
\NormalTok{n }\OperatorTok{=} \DecValTok{25}

\NormalTok{h }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n}\OperatorTok{=}\NormalTok{n, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{activities }\OperatorTok{=}\NormalTok{ h.get\_activities(exact}\OperatorTok{=}\NormalTok{exact)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Mean activity: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean(activities)}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}
    \SpecialStringTok{f"Normalized average sensitivity: "}
    \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{h}\SpecialCharTok{.}\NormalTok{get\_average\_sensitivity(exact}\OperatorTok{=}\NormalTok{exact)}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Mean activity: 0.4997
Normalized average sensitivity: 0.5003
\end{verbatim}

\textbf{Interpretation}

Random Boolean functions satisfy:

\begin{itemize}
\tightlist
\item
  mean activity \(\approx 0.5\),
\item
  normalized average sensitivity \(\approx 0.5\).
\end{itemize}

Thus, the results for \texttt{h} align with known theoretical results.
More generally, random Boolean function results define the typical
behavior against which biological functions can be compared (see
Tutorial 5).

\subsection{Summary}\label{summary}

In this tutorial you learned:

\begin{itemize}
\tightlist
\item
  how to compute symmetry groups,
\item
  how to test for input degeneracy,
\item
  how to compute activities and sensitivities,
\item
  how these quantities relate to robustness and structure.
\end{itemize}

These concepts provide essential foundations for understanding

\begin{itemize}
\tightlist
\item
  canalization, the core concept of Tutorial 3,
\item
  and the robustness of Boolean networks, explored in Tutorial 8.
\end{itemize}

\section{Canalization}\label{canalization}

Canalization is a key property of biological Boolean functions that
confers robustness: when a canalizing variable takes its canalizing
value, the output is determined regardless of other inputs. This
``buffering'' mechanism is thought to protect organisms from genetic and
environmental perturbations.

Discovered by C.H. Waddington in 1942 in developmental biology,
canalization has since been formalized in Boolean network theory and
found to be prevalent in empirically-derived gene regulatory networks.

\subsection{What you will learn}\label{what-you-will-learn-2}

In this tutorial you will:

\begin{itemize}
\tightlist
\item
  determine if a Boolean function is canalizing, \(k\)-canalizing, and
  nested canalizing,
\item
  compute the canalizing layer structure of any Boolean function,
\item
  compute properties related to collective canalization, such as
  canalizing strength, effective degree and input redundancy.
\end{itemize}

\subsection{Setup}\label{setup-2}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{Canalizing variables and
layers}\label{canalizing-variables-and-layers}

A Boolean function \(f(x_1, \ldots, x_n)\) is \emph{canalizing} if there
exists at least one \emph{canalizing variable} \(x_i\) and a
\emph{canalizing input value} \(a \in \{0,1\}\) such that

\[
f(x_1,\ldots,x_i=a,\ldots,x_n)=b,
\]

where \(b \in \{0,1\}\) is a constant, the \emph{canalized output}.

A Boolean function is \emph{k-canalizing} if it has at least k
conditionally canalizing variables. This is checked recursively: after
fixing a canalizing variable \(x_i\) to its non-canalizing input value
\(\bar a\), the subfunction \(f(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n)\)
must itself contain another canalizing variable, and so on. For a given
function, the maximal possible value of k is defined as its
\emph{canalizing depth}. If all variables are conditionally canalizing
(i.e., if the canalizing depth is \(n\)), the function is called a
\emph{nested canalizing} function (\emph{NCF}). Biological networks are
heavily enriched for NCFs as we explore in a later tutorial.

Per (He and Macauley, Physica D, 2016), any Boolean function can be
decomposed into a unique standard monomial form by recursively
identifying and removing all conditionally canalizing variables (this
set of variables is called a \emph{canalizing layer}). Each variable of
a Boolean function appears in exactly one layer, or (if it is not
conditionally canalizing) it is part of the non-canalizing core function
that has to be evaluated only if all conditionally canalizing variables
receive their non-canalizing input value. The \emph{canalizing layer
structure} \([k_1,\ldots,k_r]\) describes the number of variables in
each canalizing layer. We thus have \(r\geq 0\), \(k_i\geq 1\) and
\(k_1+\cdots+k_r\).

In the following code, we define four 3-input functions with different
canalizing properties.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Non{-}canalizing XOR function}
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"(x0 + x1 + x2) \% 2"}\NormalTok{)}

\CommentTok{\# 1{-}canalizing function}
\NormalTok{g }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"(x0 | (x1 \& x2 | \textasciitilde{}x1 \& \textasciitilde{}x2)) \% 2"}\NormalTok{)}

\CommentTok{\# Nested canalizing function with all variables in one layer}
\NormalTok{h }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"\textasciitilde{}x0 \& x1 \& x2"}\NormalTok{)}

\CommentTok{\# Nested canalizing function with two canalizing layers}
\NormalTok{k }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(}\StringTok{"x0 | (x1 \& x2)"}\NormalTok{)}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"f"}\NormalTok{, }\StringTok{"g"}\NormalTok{, }\StringTok{"h"}\NormalTok{, }\StringTok{"k"}\NormalTok{]}
\NormalTok{boolforge.display\_truth\_table(f, g, h, k, labels}\OperatorTok{=}\NormalTok{labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0  x1  x2  |   f   g   h   k
---------------------------------------------------------
0   0   0   |   0   1   0   0
0   0   1   |   1   0   0   0
0   1   0   |   1   0   0   0
0   1   1   |   1   1   1   1
1   0   0   |   1   1   0   1
1   0   1   |   1   1   0   1
1   1   0   |   1   1   0   1
1   1   1   |   1   1   0   1
\end{verbatim}

\subsubsection{Canalizing depth and nested
canalization}\label{canalizing-depth-and-nested-canalization}

For each function, we can determine whether it is canalizing and/or
nested canalizing. This is determined by the canalizing depth (the
number of conditionally canalizing variables), which we can also
directly compute. As a reminder, an \(n\)-input function is canalizing
if its canalizing depth is non-zero and nested canalizing if its
canalizing depth equals \(n\).

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h, k], labels):}
\NormalTok{    depth }\OperatorTok{=}\NormalTok{ func.get\_canalizing\_depth()}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Canalizing depth of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{depth}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{ is canalizing:"}\NormalTok{, func.is\_canalizing())}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{ is nested canalizing:"}\NormalTok{, func.is\_k\_canalizing(k}\OperatorTok{=}\NormalTok{func.n))}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Canalizing depth of f: 3
f is canalizing: True
f is nested canalizing: True

Canalizing depth of g: 1
g is canalizing: True
g is nested canalizing: False

Canalizing depth of h: 3
h is canalizing: True
h is nested canalizing: True

Canalizing depth of k: 3
k is canalizing: True
k is nested canalizing: True
\end{verbatim}

\subsubsection{Canalizing layer
structure}\label{canalizing-layer-structure}

The full canalizing layer structure includes canalizing input values,
canalized output values, the order of canalizing variables, the layer
structure, and the remaining non-canalizing core function.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h, k], labels):}
\NormalTok{    info }\OperatorTok{=}\NormalTok{ func.get\_layer\_structure()}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Canalizing input values of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}CanalizingInputs\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Canalized output values of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}CanalizedOutputs\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Order of canalizing variables of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}OrderOfCanalizingVariables\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Layer structure of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}LayerStructure\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Number of layers of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}NumberOfLayers\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Core function of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}CoreFunction\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Canalizing input values of f: [1 1 1]
Canalized output values of f: [1 1 1]
Order of canalizing variables of f: [0 1 2]
Layer structure of f: [3]
Number of layers of f: 1
Core function of f: [0]

Canalizing input values of g: [1]
Canalized output values of g: [1]
Order of canalizing variables of g: [0]
Layer structure of g: [1]
Number of layers of g: 1
Core function of g: [1 0 0 1]

Canalizing input values of h: [1 0 0]
Canalized output values of h: [0 0 0]
Order of canalizing variables of h: [0 1 2]
Layer structure of h: [3]
Number of layers of h: 1
Core function of h: [1]

Canalizing input values of k: [1 0 0]
Canalized output values of k: [1 0 0]
Order of canalizing variables of k: [0 1 2]
Layer structure of k: [1, 2]
Number of layers of k: 2
Core function of k: [1]
\end{verbatim}

Consider, for example, the output for \texttt{k}. The canalizing input
values corresponding to \(x_0, x_1, x_2\) are \(1,0,0\), respectively,
with the same canalized outputs. That is,

\begin{itemize}
\tightlist
\item
  Layer 1: \(x_0\) (if \(x_0=1\), then \(k=1\), regardless of \(x_1\)
  and \(x_2\))
\item
  Layer 2: \(x_1, x_2\) (if \(x_0=0\) and \(x_1=0\) or \(x_2=0\), then
  \(k=0\))
\end{itemize}

\subsection{Collective canalization}\label{collective-canalization}

Collective canalization treats canalization as a property of the
function rather than individual variables (Reichhardt \& Bassler, J.
Phys. A, 2007).

A Boolean function is \emph{\(k\)-set canalizing} if there exists a set
of \(k\) variables whose fixed values determine the output irrespective
of the remaining inputs.

Individual canalization asks: ``Which single variables can determine
output?'' Collective canalization asks: ``Which \emph{sets} of variables
can determine output?''

A 2-set canalizing example: If \(k(x_0,x_1,x_2) = x_0 \| (x_1 \& x_2)\),
- \(\{x_0,x_1\}\) can determine the output: if \((x_0,x_1)=(1,0)\),
\(k=1\) (\(x_2\) irrelevant), - \(\{x_1,x_2\}\) can determine the
output: if \((x_1,x_2)=(1,1)\), \(k=1\) (\(x_0\) irrelevant)

The proportion of such \(k\)-sets, the \(k\)-set canalizing proportion
denoted \(P_k(f)\), is used to define the canalizing strength. It is
fairly obvious that

\begin{itemize}
\tightlist
\item
  nested canalizing functions of a single layer such as \texttt{h} are
  the non-degenerate functions with highest k-set canalizing proportion
  \(P_k(f) = 1-1/2^k\), and
\item
  \(P_{k-1}(f) \leq P_k(f)\), i.e., more knowledge about a function's
  inputs cannot result in less knowledge about its output,
\item
  the \(n-1\)-set canalizing proportion \(P_{n-1}(f)\) is 1 minus the
  function's normalized average sensitivity.
\end{itemize}

We can compute the \(k\)-set canalizing proportions for the four 3-input
functions:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h, k], labels):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"1{-}set canalizing proportion of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_kset\_canalizing\_proportion(k}\OperatorTok{=}\DecValTok{1}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"2{-}set canalizing proportion of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_kset\_canalizing\_proportion(k}\OperatorTok{=}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Normalized average sensitivity of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_average\_sensitivity(exact}\OperatorTok{=}\VariableTok{True}\NormalTok{, normalized}\OperatorTok{=}\VariableTok{True}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"3{-}set canalizing proportion of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_kset\_canalizing\_proportion(k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1-set canalizing proportion of f: 0.5
2-set canalizing proportion of f: 0.75
Normalized average sensitivity of f: 0.25
3-set canalizing proportion of f: 1.0

1-set canalizing proportion of g: 0.16666666666666666
2-set canalizing proportion of g: 0.5
Normalized average sensitivity of g: 0.5
3-set canalizing proportion of g: 1.0

1-set canalizing proportion of h: 0.5
2-set canalizing proportion of h: 0.75
Normalized average sensitivity of h: 0.25
3-set canalizing proportion of h: 1.0

1-set canalizing proportion of k: 0.16666666666666666
2-set canalizing proportion of k: 0.5833333333333334
Normalized average sensitivity of k: 0.4166666666666667
3-set canalizing proportion of k: 1.0
\end{verbatim}

\subsubsection{Canalizing strength}\label{canalizing-strength}

The \emph{canalizing strength} summarizes collective canalization as a
weighted average of the \(k\)-set canalizing proportions (Kadelka et
al., Adv Appl Math, 2023). It ranges from:

\begin{itemize}
\tightlist
\item
  1 for maximally canalizing non-degenerate functions (namely, nested
  canalizing functions of a single canalizing layer such as \texttt{h}),
\item
  0 for linear functions such as \texttt{f},
\end{itemize}

For all other non-degenerate Boolean functions it is within \((0,1)\).

It helps to consider the canalizing strength as a probability: Given
that I know a random number of function inputs (drawn uniformly at
random from \(1,\ldots,n-1\)), how likely am I to already know the
function output?

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h, k], labels):}
\NormalTok{    strength }\OperatorTok{=}\NormalTok{ func.get\_canalizing\_strength()}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Canalizing strength of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{strength}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Canalizing strength of f: 1.0

Canalizing strength of g: 0.5

Canalizing strength of h: 1.0

Canalizing strength of k: 0.5555555555555556
\end{verbatim}

\subsubsection{Distribution of canalizing
strength}\label{distribution-of-canalizing-strength}

An enumeration of all non-degenerate 3-input Boolean functions reveals
the distribution of the canalizing strength. Note that this brute-force
code can also run (in less than a minute) for all
\(2^{2^4}=2^{16}=65,536\) 4-input functions but will take days for all
\(2^{2^5}=2^{32}=4,294,967,296\) 5-input functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{3}
\NormalTok{all\_functions }\OperatorTok{=}\NormalTok{ boolforge.get\_left\_side\_of\_truth\_table(}\DecValTok{2}\OperatorTok{**}\NormalTok{n)}

\NormalTok{canalizing\_strengths }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ binary\_vector }\KeywordTok{in}\NormalTok{ all\_functions:}
\NormalTok{    func }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(f}\OperatorTok{=}\NormalTok{binary\_vector)}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ func.is\_degenerate():}
\NormalTok{        canalizing\_strengths.append(func.get\_canalizing\_strength())}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{ax.hist(canalizing\_strengths, bins}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Canalizing strength"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Count"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial03_canalization_files/tutorial03_canalization_14_0.png}}
\caption{png}
\end{figure}

\subsection{Canalization as a measure of input
redundancy}\label{canalization-as-a-measure-of-input-redundancy}

Canalization, symmetry and redundancy are related concepts. A highly
symmetry Boolean function with few (e.g., one) symmetry groups exhibits
high input redundancy and is on average more canalizing, irrespective of
the measure of canalization. Recently, it was shown that almost all
Boolean functions (except the linear functions) exhibit some level of
\emph{input redundancy} (Gates et al., PNAS, 2021). The input redundancy
of a variable is defined as 1 minus its \emph{edge effectiveness}, which
describes the proportion of times that this variable is needed to
determine the output of the function. Edge effectiveness is very similar
to the activity of a variable but is not the same (the difference is
defined as \emph{excess canalization}). The sum of all edge
effectivenesses of the inputs of a function is known as its
\emph{effective degree}. The average input redundancy serves as a
measure of the canalization in a function.

In \texttt{BoolForge}, all these quantities can be computed, however not
directly. Instead, they are computed using the \texttt{CANA} package,
which must be installed (\texttt{pip\ install\ cana}) to enjoy this
functionality. To exemplify this, we reconsider the four 3-input
functions from above.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h, k], labels):}
\NormalTok{    edge\_eff }\OperatorTok{=}\NormalTok{ func.get\_edge\_effectiveness()}
\NormalTok{    activities }\OperatorTok{=}\NormalTok{ func.get\_activities()}
\NormalTok{    effective\_degree }\OperatorTok{=}\NormalTok{ func.get\_effective\_degree()}
\NormalTok{    input\_redundancy }\OperatorTok{=}\NormalTok{ func.get\_input\_redundancy()}

    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Edge effectiveness of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{edge\_eff}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Activities of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{activities}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Excess canalization of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{edge\_eff }\OperatorTok{{-}}\NormalTok{ activities}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Effective degree of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{effective\_degree}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Average edge effectiveness of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{effective\_degree }\OperatorTok{/}\NormalTok{ func}\SpecialCharTok{.}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Normalized input redundancy of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{input\_redundancy}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Edge effectiveness of f: [0.41666666666666663, 0.41666666666666663, 0.41666666666666663]
Activities of f: [0.2544 0.2649 0.2598]
Excess canalization of f: [0.16226667 0.15176667 0.15686667]
Effective degree of f: 1.25
Average edge effectiveness of f: 0.4166666666666667
Normalized input redundancy of f: 0.5833333333333334

Edge effectiveness of g: [0.625, 0.625, 0.625]
Activities of g: [0.4948 0.4878 0.4878]
Excess canalization of g: [0.1302 0.1372 0.1372]
Effective degree of g: 1.875
Average edge effectiveness of g: 0.625
Normalized input redundancy of g: 0.375

Edge effectiveness of h: [0.41666666666666663, 0.41666666666666663, 0.41666666666666663]
Activities of h: [0.248  0.2517 0.2501]
Excess canalization of h: [0.16866667 0.16496667 0.16656667]
Effective degree of h: 1.25
Average edge effectiveness of h: 0.4166666666666667
Normalized input redundancy of h: 0.5833333333333334

Edge effectiveness of k: [0.8125, 0.375, 0.375]
Activities of k: [0.7439 0.2541 0.2477]
Excess canalization of k: [0.0686 0.1209 0.1273]
Effective degree of k: 1.5625
Average edge effectiveness of k: 0.5208333333333334
Normalized input redundancy of k: 0.4791666666666667
\end{verbatim}

\subsection{Summary and next steps}\label{summary-and-next-steps}

In this tutorial you learned how to:

\begin{itemize}
\tightlist
\item
  compute canalizing depth and identify nested canalizing functions,
\item
  compute the canalizing layer structure and interpret layers and core
  functions,
\item
  quantify collective canalization via \(k\)-set canalizing proportions,
\item
  summarize canalization via canalizing strength,
\item
  relate canalization to redundancy-based measures such as edge
  effectiveness.
\end{itemize}

Canalization provides a structural explanation for why many biological
Boolean rules are robust to perturbations.

\textbf{Next steps:} Subsequent tutorials will explore random Boolean
functions with prescribed canalization properties and the impact of
canalization on Boolean network dynamics and robustness.

\section{Random Boolean Function
Generation}\label{random-boolean-function-generation}

This tutorial focuses on the random generation of Boolean functions with
prescribed properties, enabling large-scale computational studies.

Random Boolean function generation enables: 1. Null model comparisons:
Are biological networks special? 2. Ensemble studies: How do structural
properties affect dynamics? 3. Robustness testing: Sample the space of
equivalent models 4. Theoretical predictions: Derive expected values for
network properties

\subsection{What you will learn}\label{what-you-will-learn-3}

In this tutorial you will learn how to generate random Boolean functions
with:

\begin{itemize}
\tightlist
\item
  specified canalizing properties (depth, layer structure),
\item
  bias, absolute bias, or a specific Hamming weight,
\item
  linearity constraints,
\item
  degeneracy constraints.
\end{itemize}

It is strongly recommended to complete the previous tutorials first.

\subsection{Setup}\label{setup-3}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{Generating random Boolean
functions}\label{generating-random-boolean-functions}

The function \texttt{boolforge.random\_function(n,\ *args)} generates a
random \(n\)-input Boolean function subject to optional constraints. By
default, it generates a \textbf{non-degenerate} function, meaning that
all variables are essential.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{3}
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n)}

\NormalTok{boolforge.display\_truth\_table(f, labels}\OperatorTok{=}\StringTok{"f\_random\_non\_degenerate"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Is f degenerate?"}\NormalTok{, f.is\_degenerate())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Activities of f:"}\NormalTok{, f.get\_activities(exact}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Edge effectiveness of f:"}\NormalTok{, f.get\_edge\_effectiveness())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0  x1  x2  |   f_random_non_degenerate
-------------------------------------------------------
0   0   0   |   1
0   0   1   |   0
0   1   0   |   0
0   1   1   |   0
1   0   0   |   1
1   0   1   |   1
1   1   0   |   1
1   1   1   |   0
Is f degenerate? False
Activities of f: [0.5 0.5 0.5]
Edge effectiveness of f: [0.6666666666666667, 0.6666666666666667, 0.6666666666666667]
\end{verbatim}

The rest of this tutorial describes the various constraints. Each
constraint defines a specific family of n-input Boolean functions, from
which \texttt{boolforge.random\_function(n,*args)} samples
\emph{uniformly at random}. That is, each function satisfying a given
set of constraints is selected with equal probability.

\subsection{Parity functions}\label{parity-functions}

Setting \texttt{parity=True} generates \emph{parity} functions, also
known as non-degenerate \emph{linear} functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, parity}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{boolforge.display\_truth\_table(f, labels}\OperatorTok{=}\StringTok{"f\_linear"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Activities:"}\NormalTok{, f.get\_activities(exact}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Edge effectiveness:"}\NormalTok{, f.get\_edge\_effectiveness())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normalized average sensitivity:"}\NormalTok{, f.get\_average\_sensitivity(exact}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Canalizing strength:"}\NormalTok{, f.get\_canalizing\_strength())}

\CommentTok{\# Parity functions are the only Boolean functions with activity 1 (for all variables),}
\CommentTok{\# normalized average sensitivity 1 and canalizing strength 0.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0  x1  x2  |   f_linear
----------------------------------------
0   0   0   |   0
0   0   1   |   1
0   1   0   |   1
0   1   1   |   0
1   0   0   |   1
1   0   1   |   0
1   1   0   |   0
1   1   1   |   1
Activities: [1. 1. 1.]
Edge effectiveness: [1.0, 1.0, 1.0]
Normalized average sensitivity: 1.0
Canalizing strength: 0.0
\end{verbatim}

\subsection{Functions with prescribed canalizing
properties}\label{functions-with-prescribed-canalizing-properties}

If \texttt{parity=False} (default), the canalizing layer structure can
be specified via \texttt{layer\_structure}. This specifies the number of
conditionally canalizing variables in each layer of the randomly
generated function. If the optional argument \texttt{exact\_depth=True}
(default is False), then this describes the exact layer structure, i.e.,
the core function cannot be canalizing. If \texttt{exact\_depth=False}
(the default), it is possible that the core function is canalizing,
meaning that the last described layer in \texttt{layer\_structure} may
have more conditionally canalizing variables, or that there are
additional canalizing layers.

Before generating any random function, \texttt{random\_function()} goes
through a number of checks ensuring that the provided optional arguments
make sense. For example, it checks that the provided layer structure
\((k_1,\ldots,k_r)\) satisfies - \(k_i\geq 1\), -
\(k_1 + \cdots + k_r \leq n\), and - if \(k_1 + \cdots + k_r = n\), then
\(k_r \geq 2\) because the last layer of a nested canalizing function
must always contain two or more variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, layer\_structure}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{])}
\NormalTok{g }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, layer\_structure}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{], exact\_depth}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{h }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, layer\_structure}\OperatorTok{=}\NormalTok{[}\DecValTok{3}\NormalTok{])}
\NormalTok{k }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, layer\_structure}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{])}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"f"}\NormalTok{, }\StringTok{"g"}\NormalTok{, }\StringTok{"h"}\NormalTok{, }\StringTok{"k"}\NormalTok{]}
\NormalTok{boolforge.display\_truth\_table(f, g, h, k, labels}\OperatorTok{=}\NormalTok{labels)}

\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h, k], labels):}
\NormalTok{    info }\OperatorTok{=}\NormalTok{ func.get\_layer\_structure()}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Canalizing depth of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_canalizing\_depth()}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Layer structure of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}LayerStructure\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Number of layers of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}NumberOfLayers\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Core function of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{info[}\StringTok{\textquotesingle{}CoreFunction\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0  x1  x2  |   f   g   h   k
---------------------------------------------------------
0   0   0   |   0   1   0   1
0   0   1   |   0   1   0   1
0   1   0   |   0   1   1   1
0   1   1   |   0   0   0   1
1   0   0   |   0   1   0   1
1   0   1   |   1   0   0   0
1   1   0   |   1   1   0   0
1   1   1   |   1   1   0   0
Canalizing depth of f: 3
Layer structure of f: [1, 2]
Number of layers of f: 2
Core function of f: [0]

Canalizing depth of g: 1
Layer structure of g: [1]
Number of layers of g: 1
Core function of g: [1 0 0 1]

Canalizing depth of h: 3
Layer structure of h: [3]
Number of layers of h: 1
Core function of h: [1]

Canalizing depth of k: 3
Layer structure of k: [1, 2]
Number of layers of k: 2
Core function of k: [1]
\end{verbatim}

Repeated evaluation of this block of code shows that the canalizing
depth of \texttt{f} is either 1 or 3 (note that a canalizing depth of
\(n-1\) is never possible for a non-degenerate function). On the
contrary, the canalizing depth of \texttt{g} is always 1 because we set
\texttt{exact\_depth=True}. The 2-input core function of \texttt{g} is
one of the two parity functions, each with 50\% probability. Likewise,
the core function for the other functions is simply {[}0{]} or {[}1{]},
each with 50\% probability. Functions \texttt{h} and \texttt{k} are
nested canalizing, i.e., their canalizing depth is 3. Their layer
structure is exactly as specified.

If we do not care about the specific layer structure but only about the
canalizing depth, we specify the optional argument \texttt{depth}
instead of \texttt{layer\_structure}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# any function has at least canalizing depth 0 so this is the same as boolforge.random\_function(n)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n,depth}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\CommentTok{\# a random non{-}canalizing function}
\NormalTok{g }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n,depth}\OperatorTok{=}\DecValTok{0}\NormalTok{,exact\_depth}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# a random canalizing function}
\NormalTok{h }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n,depth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# a random nested canalizing function}
\NormalTok{k }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n,depth}\OperatorTok{=}\NormalTok{n)}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"f"}\NormalTok{, }\StringTok{"g"}\NormalTok{, }\StringTok{"h"}\NormalTok{, }\StringTok{"k"}\NormalTok{]}
\NormalTok{boolforge.display\_truth\_table(f, g, h, k, labels}\OperatorTok{=}\NormalTok{labels)}

\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h, k], labels):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Canalizing depth of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_canalizing\_depth()}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0  x1  x2  |   f   g   h   k
---------------------------------------------------------
0   0   0   |   0   0   1   1
0   0   1   |   0   1   0   0
0   1   0   |   1   0   1   1
0   1   1   |   0   0   1   1
1   0   0   |   0   0   0   1
1   0   1   |   1   1   0   1
1   1   0   |   0   1   0   1
1   1   1   |   1   0   0   1
Canalizing depth of f: 0

Canalizing depth of g: 0

Canalizing depth of h: 3

Canalizing depth of k: 3
\end{verbatim}

Repeated evaluation of this block of code shows that the canalizing
depth of \texttt{f} can be 0, 1, or 3. Note that specifying
\texttt{depth=0} without \texttt{exact\_depth=True} does not restrict
the space of functions at all. On the contrary, the canalizing depth of
\texttt{g} is always 0 (i.e., g does not contain any canalizing
variables) because we set \texttt{exact\_depth=True}. Function
\texttt{h} is canalizing and may be nested canalizing (because we
specified that the minimal canalizing depth is 1), and \texttt{k} is
always nested canalizing (i.e., it has canalizing depth \(n=3\)).

We remember: If \texttt{exact\_depth=True}, \texttt{depth} is
interpreted as exact canalizing depth. Otherwise (default),
\texttt{depth} is interpreted as minimal canalizing depth. For example,

\begin{itemize}
\tightlist
\item
  \texttt{depth=1}: ``At least 1-canalizing'' (could be
  2,3,\ldots,n-canalizing)
\item
  \texttt{depth=1,\ exact\_depth=True}: ``Exactly 1-canalizing'' (not
  2,3,\ldots,n-canalizing)
\end{itemize}

\subsection{Allowing degenerate
functions}\label{allowing-degenerate-functions}

It is possible that an n-input Boolean function does not depend on all
its variables. For example, the function \(f(x,y) = x\) depends on \(x\)
but not on \(y\). By default, such degenerate functions are never
generated by \texttt{boolforge.random\_function()}. To enable the
generation of possibly degenerate functions, we set
\texttt{allow\_degenerate\_functions=True}. Although hardly of any
practical value, we can even restrict the random generation to
degenerate functions only, using
\texttt{boolforge.random\_degenerate\_function(n,*args)}.

Since degenerate functions occur much more frequently at low degree, we
set \texttt{n=2}, generate a large number of random, possibly degenerate
functions and compare a histogram of the observed number of essential
variables to the expected proportions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{2}
\NormalTok{n\_simulations }\OperatorTok{=} \DecValTok{10000}

\NormalTok{count\_essential }\OperatorTok{=}\NormalTok{ np.zeros(n }\OperatorTok{+} \DecValTok{1}\NormalTok{, dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
\NormalTok{    f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    count\_essential[f.get\_number\_of\_essential\_variables()] }\OperatorTok{+=} \DecValTok{1}

\NormalTok{expected }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2} \OperatorTok{/} \DecValTok{16}\NormalTok{, }\DecValTok{4} \OperatorTok{/} \DecValTok{16}\NormalTok{, }\DecValTok{10} \OperatorTok{/} \DecValTok{16}\NormalTok{])}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(n }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{width }\OperatorTok{=} \FloatTok{0.4}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{ax.bar(x }\OperatorTok{{-}}\NormalTok{ width }\OperatorTok{/} \DecValTok{2}\NormalTok{, count\_essential }\OperatorTok{/}\NormalTok{ n\_simulations, width}\OperatorTok{=}\NormalTok{width, label}\OperatorTok{=}\StringTok{"observed"}\NormalTok{)}
\NormalTok{ax.bar(x }\OperatorTok{+}\NormalTok{ width }\OperatorTok{/} \DecValTok{2}\NormalTok{, expected, width}\OperatorTok{=}\NormalTok{width, label}\OperatorTok{=}\StringTok{"expected"}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xticks(x)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Number of essential variables"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\SpecialStringTok{f"Proportion of }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{{-}input functions"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Error:"}\NormalTok{, count\_essential }\OperatorTok{/}\NormalTok{ n\_simulations }\OperatorTok{{-}}\NormalTok{ expected)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error: [-0.0023 -0.0034  0.0057]
\end{verbatim}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial04_random_Boolean_function_generation_files/tutorial04_random_Boolean_function_generation_13_1.png}}
\caption{png}
\end{figure}

\subsection{Functions with prescribed Hamming
weight}\label{functions-with-prescribed-hamming-weight}

The Hamming weight of a Boolean function is the number of ones in its
truth table. BoolForge allows for the generation of random n-input
functions with a specific Hamming weight \(w\in\{0,1,\ldots,2^n\}\). The
additional optional parameters \texttt{allow\_degenerate\_functions} and
\texttt{exact\_depth} specify whether degenerate and canalizing
functions are allowed. By default, canalizing functions are allowed,
while degenerate functions are not. Since all functions with Hamming
weight \(w\in\{0,1,2^n-1,2^n\}\) are canalizing, we require
\(2\leq w\leq 2^n-2\) whenever canalizing functions are not permissible
(i.e., whenever\texttt{exact\_depth=True}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{3}

\NormalTok{f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, hamming\_weight}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{g }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, hamming\_weight}\OperatorTok{=}\DecValTok{5}\NormalTok{, exact\_depth}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{h }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, hamming\_weight}\OperatorTok{=}\DecValTok{2}\NormalTok{, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"f"}\NormalTok{, }\StringTok{"g"}\NormalTok{, }\StringTok{"h"}\NormalTok{]}
\NormalTok{boolforge.display\_truth\_table(f, g, h, labels}\OperatorTok{=}\NormalTok{labels)}

\ControlFlowTok{for}\NormalTok{ func, label }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{([f, g, h], labels):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Hamming weight of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_hamming\_weight()}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Canalizing depth of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_canalizing\_depth()}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Number of essential variables of }\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{func}\SpecialCharTok{.}\NormalTok{get\_number\_of\_essential\_variables()}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0  x1  x2  |   f   g   h
-------------------------------------------------
0   0   0   |   1   1   0
0   0   1   |   0   1   0
0   1   0   |   1   1   0
0   1   1   |   0   0   1
1   0   0   |   1   0   0
1   0   1   |   0   1   0
1   1   0   |   1   0   0
1   1   1   |   1   1   1
Hamming weight of f: 5
Canalizing depth of f: 3
Number of essential variables of f: 3

Hamming weight of g: 5
Canalizing depth of g: 0
Number of essential variables of g: 3

Hamming weight of h: 2
Canalizing depth of h: 2
Number of essential variables of h: 2
\end{verbatim}

\subsection{Biased and absolutely biased
functions}\label{biased-and-absolutely-biased-functions}

While specifying the Hamming weight fixes the exact number of 1s in the
truth table of a generated function, specifying the bias or absolute
bias acts slightly differently. The bias \(p\) describes the probability
of selecting a 1 at any position in the truth table and can be modified
using the optional argument \texttt{bias}. Instead of specifying the
bias, the absolute bias may also be specified. Unbiased functions
generated using \(p=0.5\) have an absolute bias of \(0\), the default.
If, for example, we set \texttt{absolute\_bias=0.5} and specify to use
absolute bias (\texttt{use\_absolute\_bias=True}, default is False), the
bias used to generate the function is either 0.25 or 0.75, both with
probability 50\%. Generally, if we set
\texttt{use\_absolute\_bias=True;\ absolute\_bias=a} for \(a\in [0,1]\),
the bias is either \((1+a)/2\) or \((1-a)/2\), both with probability
50\%.

To display these different modes, we repeatedly generate random Boolean
functions under three different constraints (\texttt{f} with bias
\(p=0.75\), \texttt{g} with absolute bias 0.5, and \texttt{h} an
unbiased function, i.e., with bias \(p=0.5\)), and compare the empirical
Hamming weight distribution of the three families of functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{4}
\NormalTok{n\_simulations }\OperatorTok{=} \DecValTok{10000}

\NormalTok{counts }\OperatorTok{=}\NormalTok{ np.zeros((}\DecValTok{3}\NormalTok{, }\DecValTok{2}\OperatorTok{**}\NormalTok{n }\OperatorTok{+} \DecValTok{1}\NormalTok{), dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
\NormalTok{    f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, bias}\OperatorTok{=}\FloatTok{0.75}\NormalTok{)}
\NormalTok{    g }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, absolute\_bias}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, use\_absolute\_bias}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    h }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, absolute\_bias}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{    counts[}\DecValTok{0}\NormalTok{, f.get\_hamming\_weight()] }\OperatorTok{+=} \DecValTok{1}
\NormalTok{    counts[}\DecValTok{1}\NormalTok{, g.get\_hamming\_weight()] }\OperatorTok{+=} \DecValTok{1}
\NormalTok{    counts[}\DecValTok{2}\NormalTok{, h.get\_hamming\_weight()] }\OperatorTok{+=} \DecValTok{1}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"bias 0.75"}\NormalTok{, }\StringTok{"absolute bias 0.5"}\NormalTok{, }\StringTok{"random"}\NormalTok{]}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{2}\OperatorTok{**}\NormalTok{n }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{width }\OperatorTok{=} \FloatTok{0.3}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\ControlFlowTok{for}\NormalTok{ i, label }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(labels):}
\NormalTok{    ax.bar(x }\OperatorTok{{-}}\NormalTok{ width }\OperatorTok{+}\NormalTok{ i }\OperatorTok{*}\NormalTok{ width, counts[i] }\OperatorTok{/}\NormalTok{ n\_simulations, width}\OperatorTok{=}\NormalTok{width, label}\OperatorTok{=}\NormalTok{label)}

\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xticks(x)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Hamming weight"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\SpecialStringTok{f"Proportion of }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{{-}input functions"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial04_random_Boolean_function_generation_files/tutorial04_random_Boolean_function_generation_17_0.png}}
\caption{png}
\end{figure}

This plot exemplifies the difference between bias and absolute bias:

\begin{itemize}
\tightlist
\item
  Specifying the bias shifts the mode of the Hamming weight distribution
  to the value of \texttt{bias}.
\item
  Specifying the absolute bias yields random functions with a bimodal
  Hamming weight distribution.
\end{itemize}

Note that \texttt{absolute\_bias=0.5} is ignored in the generation of
\texttt{h}. The desired use of absolute bias must be specified by
\texttt{use\_absolute\_bias=True}.

In the above plot, we notice a lack of functions with Hamming weight 0
and \(16=2^n\). These constant functions are degenerate and thus not
generated unless we set \texttt{allow\_degenerate\_functions=True},
which as we see below slightly modifies the resulting Hamming weight
distributions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{counts[:] }\OperatorTok{=} \DecValTok{0}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
\NormalTok{    f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, bias}\OperatorTok{=}\FloatTok{0.75}\NormalTok{, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    g }\OperatorTok{=}\NormalTok{ boolforge.random\_function(}
\NormalTok{        n, absolute\_bias}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, use\_absolute\_bias}\OperatorTok{=}\VariableTok{True}\NormalTok{, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}
\NormalTok{    )}
\NormalTok{    h }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, absolute\_bias}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{    counts[}\DecValTok{0}\NormalTok{, f.get\_hamming\_weight()] }\OperatorTok{+=} \DecValTok{1}
\NormalTok{    counts[}\DecValTok{1}\NormalTok{, g.get\_hamming\_weight()] }\OperatorTok{+=} \DecValTok{1}
\NormalTok{    counts[}\DecValTok{2}\NormalTok{, h.get\_hamming\_weight()] }\OperatorTok{+=} \DecValTok{1}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\ControlFlowTok{for}\NormalTok{ i, label }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(labels):}
\NormalTok{    ax.bar(x }\OperatorTok{{-}}\NormalTok{ width }\OperatorTok{+}\NormalTok{ i }\OperatorTok{*}\NormalTok{ width, counts[i] }\OperatorTok{/}\NormalTok{ n\_simulations, width}\OperatorTok{=}\NormalTok{width, label}\OperatorTok{=}\NormalTok{label)}

\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xticks(x)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Hamming weight"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\SpecialStringTok{f"Proportion of }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{{-}input functions"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial04_random_Boolean_function_generation_files/tutorial04_random_Boolean_function_generation_19_0.png}}
\caption{png}
\end{figure}

\subsection{Summary}\label{summary-1}

This tutorial demonstrated how BoolForge enables uniform random
generation of Boolean functions under flexible structural and
statistical constraints.

Different constraints define fundamentally different ensembles, and
being explicit about these choices is essential for a correct generation
and interpretation of computational results.

\textbf{Next steps:} In the next tutorial, these function-level
ensembles will be used to study how Boolean function structure
influences sensitivity and canalizing properties.

\subsection{Common pitfalls}\label{common-pitfalls}

\begin{itemize}
\tightlist
\item
  \texttt{absolute\_bias} has no effect unless
  \texttt{use\_absolute\_bias=True}.
\item
  \texttt{depth=0} without \texttt{exact\_depth=True} does not restrict
  the function space.
\item
  Constant functions are generated only if
  \texttt{allow\_degenerate\_functions=True}.
\item
  For larger \(n\) (for sure whenever \(n>10\)), set
  \texttt{allow\_degenerate\_functions=True} to avoid expensive
  degeneracy tests. Almost all functions in many variables are
  non-degenerate.
\end{itemize}

\section{Example Use Cases of the Random Function
Generator}\label{example-use-cases-of-the-random-function-generator}

In this tutorial, we explore example use cases of BoolForge's random
Boolean function generator. This functionality allows generating large
ensembles of Boolean functions with prescribed structural properties and
studying their statistical and dynamical characteristics.

\subsection{What you will learn}\label{what-you-will-learn-4}

In this tutorial you will learn how to:

\begin{itemize}
\tightlist
\item
  compute the prevalence of canalization, \(k\)-canalization, and nested
  canalization,
\item
  determine distributions of canalizing strength and normalized input
  redundancy,
\item
  investigate correlations between absolute bias and canalization,
\item
  generate and analyze dynamically distinct nested canalizing functions.
\end{itemize}

It is strongly recommended to complete the previous tutorials first.

\subsection{Setup}\label{setup-4}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ scipy.stats }\ImportTok{as}\NormalTok{ stats}
\end{Highlighting}
\end{Shaded}

\subsection{Prevalence of
canalization}\label{prevalence-of-canalization}

Using random sampling, we estimate how frequently Boolean functions of
degree \(n\) exhibit a given canalizing depth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_simulations }\OperatorTok{=} \DecValTok{1000}
\NormalTok{ns }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\NormalTok{canalizing\_depths }\OperatorTok{=}\NormalTok{ np.arange(}\BuiltInTok{max}\NormalTok{(ns) }\OperatorTok{+} \DecValTok{1}\NormalTok{)}

\NormalTok{count\_depths }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), }\BuiltInTok{max}\NormalTok{(ns) }\OperatorTok{+} \DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
    \ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
\NormalTok{        f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n)}
\NormalTok{        count\_depths[i, f.get\_canalizing\_depth()] }\OperatorTok{+=} \DecValTok{1}

\NormalTok{count\_depths }\OperatorTok{/=}\NormalTok{ n\_simulations}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\ControlFlowTok{for}\NormalTok{ i, depth }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(canalizing\_depths):}
\NormalTok{    ax.bar(}
\NormalTok{        ns,}
\NormalTok{        count\_depths[:, i],}
\NormalTok{        bottom}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(count\_depths[:, :i], axis}\OperatorTok{=}\DecValTok{1}\NormalTok{),}
\NormalTok{        label}\OperatorTok{=}\BuiltInTok{str}\NormalTok{(depth),}
\NormalTok{    )}

\NormalTok{ax.legend(}
\NormalTok{    frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    loc}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{    bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.1}\NormalTok{),}
\NormalTok{    ncol}\OperatorTok{=}\DecValTok{8}\NormalTok{,}
\NormalTok{    title}\OperatorTok{=}\StringTok{"canalizing depth"}\NormalTok{,}
\NormalTok{)}
\NormalTok{ax.set\_xticks(ns)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Number of essential variables"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Proportion of functions"}\NormalTok{)}
\NormalTok{plt.show()}

\NormalTok{pd.DataFrame(}
\NormalTok{    count\_depths,}
\NormalTok{    index}\OperatorTok{=}\NormalTok{[}\StringTok{"n="} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(n) }\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ ns],}
\NormalTok{    columns}\OperatorTok{=}\NormalTok{[}\StringTok{"k="} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(k) }\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in}\NormalTok{ canalizing\_depths],}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_3_0.png}}
\caption{png}
\end{figure}

k=0

k=1

k=2

k=3

k=4

k=5

k=6

n=2

0.197

0.000

0.803

0.000

0.000

0.0

0.0

n=3

0.584

0.111

0.000

0.305

0.000

0.0

0.0

n=4

0.948

0.033

0.007

0.000

0.012

0.0

0.0

n=5

1.000

0.000

0.000

0.000

0.000

0.0

0.0

n=6

1.000

0.000

0.000

0.000

0.000

0.0

0.0

We see that hardly any Boolean function with \(n\geq 5\) inputs is
canalizing, let alone nested canalizing. This makes the finding that
most Boolean functions in published Boolean gene regulatory network
models are nested canalizing very surprising (Kadelka et al., Science
Advances, 2024).

To zoom in on the few functions that are canalizing for higher \(n\), we
can simply require \texttt{depth=1} and repeat the above analysis.

\subsubsection{Restricting to canalizing
functions}\label{restricting-to-canalizing-functions}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{count\_depths }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), }\BuiltInTok{max}\NormalTok{(ns) }\OperatorTok{+} \DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
    \ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
\NormalTok{        f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, depth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{        count\_depths[i, f.get\_canalizing\_depth()] }\OperatorTok{+=} \DecValTok{1}

\NormalTok{count\_depths }\OperatorTok{/=}\NormalTok{ n\_simulations}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\ControlFlowTok{for}\NormalTok{ i, depth }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(canalizing\_depths):}
\NormalTok{    ax.bar(}
\NormalTok{        ns,}
\NormalTok{        count\_depths[:, i],}
\NormalTok{        bottom}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(count\_depths[:, :i], axis}\OperatorTok{=}\DecValTok{1}\NormalTok{),}
\NormalTok{        label}\OperatorTok{=}\BuiltInTok{str}\NormalTok{(depth),}
\NormalTok{    )}

\NormalTok{ax.legend(}
\NormalTok{    frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    loc}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{    bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.1}\NormalTok{),}
\NormalTok{    ncol}\OperatorTok{=}\DecValTok{8}\NormalTok{,}
\NormalTok{    title}\OperatorTok{=}\StringTok{"canalizing depth"}\NormalTok{,}
\NormalTok{)}
\NormalTok{ax.set\_xticks(ns)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Number of essential variables"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Proportion of functions"}\NormalTok{)}
\NormalTok{plt.show()}

\NormalTok{pd.DataFrame(}
\NormalTok{    count\_depths,}
\NormalTok{    index}\OperatorTok{=}\NormalTok{[}\StringTok{"n="} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(n) }\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ ns],}
\NormalTok{    columns}\OperatorTok{=}\NormalTok{[}\StringTok{"k="} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(k) }\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in}\NormalTok{ canalizing\_depths],}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_6_0.png}}
\caption{png}
\end{figure}

k=0

k=1

k=2

k=3

k=4

k=5

k=6

n=2

0.0

0.000

1.000

0.000

0.00

0.000

0.0

n=3

0.0

0.253

0.000

0.747

0.00

0.000

0.0

n=4

0.0

0.688

0.092

0.000

0.22

0.000

0.0

n=5

0.0

0.965

0.028

0.002

0.00

0.005

0.0

n=6

0.0

1.000

0.000

0.000

0.00

0.000

0.0

This analysis reveals that among Boolean functions of degree
\(n\geq 5\), functions with few conditionally canalizing variables are
much more abundant than functions with more conditionally canalizing
variables, which is mathematically obvious due to the recursive nature
of the definition of k-canalization.

\subsection{Collective canalization vs
degree}\label{collective-canalization-vs-degree}

Using a similar setup, we can investigate if and how the various
measures of collective canalization, specifically canalizing strength
(Kadelka et al., Adv in Appl Math, 2023) and the normalized input
redundancy (Gates et al., PNAS, 2021), change when the degree of the
functions changes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_simulations }\OperatorTok{=} \DecValTok{100}
\NormalTok{ns }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{)}

\NormalTok{canalizing\_strengths }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), n\_simulations))}
\NormalTok{input\_redundancies }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), n\_simulations))}

\ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
    \ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
\NormalTok{        f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n)}
\NormalTok{        canalizing\_strengths[i, j] }\OperatorTok{=}\NormalTok{ f.get\_canalizing\_strength()}
\NormalTok{        input\_redundancies[i, j] }\OperatorTok{=}\NormalTok{ f.get\_input\_redundancy()}

\NormalTok{width }\OperatorTok{=} \FloatTok{0.4}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\NormalTok{ax.violinplot(}
\NormalTok{    canalizing\_strengths.T,}
\NormalTok{    positions}\OperatorTok{=}\NormalTok{ns }\OperatorTok{{-}}\NormalTok{ width }\OperatorTok{/} \DecValTok{2}\NormalTok{,}
\NormalTok{    widths}\OperatorTok{=}\NormalTok{width,}
\NormalTok{    showmeans}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    showextrema}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{)}
\NormalTok{ax.scatter([], [], color}\OperatorTok{=}\StringTok{"C0"}\NormalTok{, label}\OperatorTok{=}\StringTok{"canalizing strength"}\NormalTok{)}

\NormalTok{ax.violinplot(}
\NormalTok{    input\_redundancies.T,}
\NormalTok{    positions}\OperatorTok{=}\NormalTok{ns }\OperatorTok{+}\NormalTok{ width }\OperatorTok{/} \DecValTok{2}\NormalTok{,}
\NormalTok{    widths}\OperatorTok{=}\NormalTok{width,}
\NormalTok{    showmeans}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    showextrema}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{)}
\NormalTok{ax.scatter([], [], color}\OperatorTok{=}\StringTok{"C1"}\NormalTok{, label}\OperatorTok{=}\StringTok{"normalized input redundancy"}\NormalTok{)}

\NormalTok{ax.legend(}
\NormalTok{    loc}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{    bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.05}\NormalTok{),}
\NormalTok{    frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    ncol}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Number of essential variables"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Value"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_9_0.png}}
\caption{png}
\end{figure}

Both measures decrease with increasing degree, but canalizing strength
declines more sharply.

If we stratify this analysis by canalizing depth (exact canalizing depth
using \texttt{exact\_depth=True} or minimal canalizing depth using the
default \texttt{exact\_depth=False}), we can confirm that functions with
more conditionally canalizing variables tend to also have higher average
collective canalization, irrespective of how it is measured. In other
words, the various measures of canalization are all highly correlated.

\subsubsection{Stratification by canalizing
depth}\label{stratification-by-canalizing-depth}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_simulations }\OperatorTok{=} \DecValTok{100}
\NormalTok{exact\_depth }\OperatorTok{=} \VariableTok{False}
\NormalTok{ns }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{)}

\NormalTok{max\_depth }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(ns)}

\NormalTok{canalizing\_strengths }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), max\_depth }\OperatorTok{+} \DecValTok{1}\NormalTok{, n\_simulations))}
\NormalTok{input\_redundancies }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), max\_depth }\OperatorTok{+} \DecValTok{1}\NormalTok{, n\_simulations))}

\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
    \ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
        \ControlFlowTok{for}\NormalTok{ depth }\KeywordTok{in}\NormalTok{ np.append(np.arange(n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{), n):}
\NormalTok{            f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, depth}\OperatorTok{=}\NormalTok{depth, exact\_depth}\OperatorTok{=}\NormalTok{exact\_depth)}
\NormalTok{            canalizing\_strengths[i, depth, k] }\OperatorTok{=}\NormalTok{ f.get\_canalizing\_strength()}
\NormalTok{            input\_redundancies[i, depth, k] }\OperatorTok{=}\NormalTok{ f.get\_input\_redundancy()}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{base\_gap }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{intra\_gap }\OperatorTok{=} \FloatTok{0.3}
\NormalTok{width }\OperatorTok{=} \FloatTok{0.28}

\ControlFlowTok{for}\NormalTok{ ii, (data, label) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(}
    \BuiltInTok{zip}\NormalTok{(}
\NormalTok{        [canalizing\_strengths, input\_redundancies],}
\NormalTok{        [}\StringTok{"canalizing strength"}\NormalTok{, }\StringTok{"normalized input redundancy"}\NormalTok{],}
\NormalTok{    )}
\NormalTok{):}
\NormalTok{    positions }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    values }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    colors }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    group\_centers }\OperatorTok{=}\NormalTok{ []}

\NormalTok{    current\_x }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
\NormalTok{        depths }\OperatorTok{=}\NormalTok{ np.append(np.arange(n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{), n)}
\NormalTok{        offsets }\OperatorTok{=}\NormalTok{ np.linspace(}
            \OperatorTok{{-}}\NormalTok{(}\BuiltInTok{len}\NormalTok{(depths) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ intra\_gap }\OperatorTok{/} \DecValTok{2}\NormalTok{,}
\NormalTok{            (}\BuiltInTok{len}\NormalTok{(depths) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ intra\_gap }\OperatorTok{/} \DecValTok{2}\NormalTok{,}
            \BuiltInTok{len}\NormalTok{(depths),}
\NormalTok{        )}
\NormalTok{        group\_positions }\OperatorTok{=}\NormalTok{ current\_x }\OperatorTok{+}\NormalTok{ offsets}
\NormalTok{        positions.extend(group\_positions)}
\NormalTok{        group\_centers.append(current\_x)}

        \ControlFlowTok{for}\NormalTok{ d }\KeywordTok{in}\NormalTok{ depths:}
\NormalTok{            values.append(data[i, d, :])}
\NormalTok{            colors.append(}\StringTok{"C"} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(d))}

\NormalTok{        group\_width }\OperatorTok{=}\NormalTok{ (}\BuiltInTok{len}\NormalTok{(depths) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ intra\_gap}
\NormalTok{        current\_x }\OperatorTok{+=}\NormalTok{ group\_width }\OperatorTok{/} \DecValTok{2} \OperatorTok{+}\NormalTok{ base\_gap }\OperatorTok{+}\NormalTok{ width }\OperatorTok{+}\NormalTok{ intra\_gap}

    \ControlFlowTok{for}\NormalTok{ pos, val, c }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(positions, values, colors):}
\NormalTok{        vp }\OperatorTok{=}\NormalTok{ ax[ii].violinplot(val, positions}\OperatorTok{=}\NormalTok{[pos], widths}\OperatorTok{=}\NormalTok{width, showmeans}\OperatorTok{=}\VariableTok{True}\NormalTok{, showextrema}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \ControlFlowTok{for}\NormalTok{ body }\KeywordTok{in}\NormalTok{ vp[}\StringTok{"bodies"}\NormalTok{]:}
\NormalTok{            body.set\_facecolor(c)}
\NormalTok{            body.set\_alpha(}\FloatTok{0.85}\NormalTok{)}
\NormalTok{        vp[}\StringTok{"cmeans"}\NormalTok{].set\_color(}\StringTok{"k"}\NormalTok{)}

\NormalTok{    ax[ii].set\_ylabel(label)}
\NormalTok{    ax[ii].set\_ylim([}\OperatorTok{{-}}\FloatTok{0.02}\NormalTok{, }\FloatTok{1.02}\NormalTok{])}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_xlabel(}\StringTok{"Number of essential variables (n)"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_xticks(group\_centers)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_xticklabels(ns)}

\NormalTok{depth\_handles }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    plt.Line2D([}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{"C"} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(d), lw}\OperatorTok{=}\DecValTok{5}\NormalTok{, label}\OperatorTok{=}\BuiltInTok{str}\NormalTok{(d))}
    \ControlFlowTok{for}\NormalTok{ d }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(max\_depth }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{]}

\NormalTok{fig.legend(}
\NormalTok{    handles}\OperatorTok{=}\NormalTok{depth\_handles,}
\NormalTok{    loc}\OperatorTok{=}\StringTok{"upper center"}\NormalTok{,}
\NormalTok{    ncol}\OperatorTok{=}\DecValTok{7}\NormalTok{,}
\NormalTok{    frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    title}\OperatorTok{=}\StringTok{"exact canalizing depth"} \ControlFlowTok{if}\NormalTok{ exact\_depth }\ControlFlowTok{else} \StringTok{"minimal canalizing depth"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_12_0.png}}
\caption{png}
\end{figure}

\subsubsection{Correlation between canalizing strength and input
redundancy}\label{correlation-between-canalizing-strength-and-input-redundancy}

We can generate all (non-degenerate) Boolean functions of a certain
degree \(n\) (only feasible up to \(n=4\)) and compare canalizing
strength and input redundancy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{3}
\NormalTok{allow\_degenerate\_functions }\OperatorTok{=} \VariableTok{False}
\NormalTok{degenerate }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{**}\NormalTok{ (}\DecValTok{2}\OperatorTok{**}\NormalTok{n), dtype}\OperatorTok{=}\BuiltInTok{bool}\NormalTok{)}
\NormalTok{strengths }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{**}\NormalTok{ (}\DecValTok{2}\OperatorTok{**}\NormalTok{n))}
\NormalTok{redundancies }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{**}\NormalTok{ (}\DecValTok{2}\OperatorTok{**}\NormalTok{n))}

\ControlFlowTok{for}\NormalTok{ i, fvec }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(boolforge.get\_left\_side\_of\_truth\_table(}\DecValTok{2}\OperatorTok{**}\NormalTok{n)):}
\NormalTok{    bf }\OperatorTok{=}\NormalTok{ boolforge.BooleanFunction(fvec)}
\NormalTok{    strengths[i] }\OperatorTok{=}\NormalTok{ bf.get\_canalizing\_strength()}
\NormalTok{    redundancies[i] }\OperatorTok{=}\NormalTok{ bf.get\_input\_redundancy()}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ allow\_degenerate\_functions:}
\NormalTok{        degenerate[i] }\OperatorTok{=}\NormalTok{ bf.is\_degenerate()}
        
\ControlFlowTok{if}\NormalTok{ allow\_degenerate\_functions:}
\NormalTok{    which }\OperatorTok{=}\NormalTok{ np.ones(}\DecValTok{2} \OperatorTok{**}\NormalTok{ (}\DecValTok{2}\OperatorTok{**}\NormalTok{n), dtype}\OperatorTok{=}\BuiltInTok{bool}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    which }\OperatorTok{=} \OperatorTok{\textasciitilde{}}\NormalTok{degenerate}
    

\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{plt.scatter(strengths[which], redundancies[which], alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Canalizing strength"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Normalized input redundancy"}\NormalTok{)}
\NormalTok{plt.title(}\SpecialStringTok{f"n = }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}

\NormalTok{stats.spearmanr(strengths[which], redundancies[which])}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_14_0.png}}
\caption{png}
\end{figure}

\begin{verbatim}
SignificanceResult(statistic=np.float64(0.9700304760700043), pvalue=np.float64(1.0759731607818433e-134))
\end{verbatim}

Both measures are highly correlated but markedly not the same. Some
functions possess relatively high canalizing strength but low input
redundancy, and vice versa. It remains an open question what drives this
behavior.

\subsection{Correlation between canalization and
bias}\label{correlation-between-canalization-and-bias}

Basically all metrics used to assess the sensitivity of Boolean
functions (canalization, absolute bias, average sensitivity) are
correlated. For example, functions with higher absolute bias are more
likely to be canalizing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{n\_simulations }\OperatorTok{=} \DecValTok{3000}
\NormalTok{bias\_values }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{21}\NormalTok{)}

\NormalTok{count\_canalizing }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), }\BuiltInTok{len}\NormalTok{(bias\_values)), dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
        \ControlFlowTok{for}\NormalTok{ j, bias }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(bias\_values):}
\NormalTok{            f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, bias}\OperatorTok{=}\NormalTok{bias, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
            \ControlFlowTok{if}\NormalTok{ f.is\_canalizing():}
\NormalTok{                count\_canalizing[i, j] }\OperatorTok{+=} \DecValTok{1}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
\NormalTok{    ax.plot(bias\_values, count\_canalizing[i] }\OperatorTok{/}\NormalTok{ n\_simulations, label}\OperatorTok{=}\SpecialStringTok{f"n=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{xticks }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{ax.set\_xticks(xticks)}
\NormalTok{ax.set\_xticklabels([}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{ (}\SpecialCharTok{\{}\BuiltInTok{round}\NormalTok{(}\DecValTok{200}\OperatorTok{*}\BuiltInTok{abs}\NormalTok{(p}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{))}\SpecialCharTok{\}}\SpecialStringTok{\%)"} \ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ xticks])}
\NormalTok{ax.set\_xlabel(}\StringTok{"bias (absolute bias)"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"probability canalizing"}\NormalTok{)}
\NormalTok{ax.legend(}
\NormalTok{    loc}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{    frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.05}\NormalTok{),}
\NormalTok{    ncol}\OperatorTok{=}\DecValTok{6}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_17_0.png}}
\caption{png}
\end{figure}

\subsubsection{Degeneracy vs bias}\label{degeneracy-vs-bias}

Similarly, the probability that a function is degenerate (i.e., that it
does not depend on all its variables) also increases as the absolute
bias increases.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{count\_degenerate }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(ns), }\BuiltInTok{len}\NormalTok{(bias\_values)), dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_simulations):}
        \ControlFlowTok{for}\NormalTok{ j, bias }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(bias\_values):}
\NormalTok{            f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, bias}\OperatorTok{=}\NormalTok{bias, allow\_degenerate\_functions}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
            \ControlFlowTok{if}\NormalTok{ f.is\_degenerate():}
\NormalTok{                count\_degenerate[i, j] }\OperatorTok{+=} \DecValTok{1}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\ControlFlowTok{for}\NormalTok{ i, n }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ns):}
\NormalTok{    ax.plot(bias\_values, count\_degenerate[i] }\OperatorTok{/}\NormalTok{ n\_simulations, label}\OperatorTok{=}\SpecialStringTok{f"n=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{ax.set\_xticks(xticks)}
\NormalTok{ax.set\_xticklabels([}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{ (}\SpecialCharTok{\{}\BuiltInTok{round}\NormalTok{(}\DecValTok{200}\OperatorTok{*}\BuiltInTok{abs}\NormalTok{(p}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{))}\SpecialCharTok{\}}\SpecialStringTok{\%)"} \ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ xticks])}
\NormalTok{ax.set\_xlabel(}\StringTok{"bias (absolute bias)"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"probability degenerate"}\NormalTok{)}
\NormalTok{ax.legend(}
\NormalTok{    loc}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{    frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.05}\NormalTok{),}
\NormalTok{    ncol}\OperatorTok{=}\DecValTok{6}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_19_0.png}}
\caption{png}
\end{figure}

\subsection{Analyzing functions with specific canalizing layer
structure}\label{analyzing-functions-with-specific-canalizing-layer-structure}

The average sensitivity of the Boolean functions governing the updates
in a Boolean network, determines the stability of the network to
perturbations. More generally, it determines the dynamical regime of the
network (see Tutorial 8). The ability to generate canalizing functions
with a specific canalizing layer structure enables us to investigate the
link between layer structure and average sensitivity, as well as other
properties, such as canalizing strength or effective degree.

For nested canalizing functions of a given degree \(n\), there exists a
bijection between their absolute bias and their canalizing layer
structure (Kadelka et al., Physica D, 2017). The function
\texttt{boolforge.utils.hamming\_weight\_to\_ncf\_layer\_structure(degree,hamming\_weight)}
implements this. NCFs with the same layer structure have the same
dynamical properties. That is, they have the same average sensitivity,
canalizing strength and the same effective degree. Iterating over all
possible absolute biases (parametrized by the possible Hamming weights),
we can thus generate all dynamically different types of n-input NCFs and
investigate their average sensitivity, which we can compute exactly for
relatively low degree.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{5}
\NormalTok{all\_hamming }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, }\DecValTok{2} \OperatorTok{**}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{all\_abs\_bias }\OperatorTok{=}\NormalTok{ all\_hamming }\OperatorTok{/} \DecValTok{2} \OperatorTok{**}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}

\NormalTok{avg\_sens }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{**}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{2}\NormalTok{))}
\NormalTok{can\_strength }\OperatorTok{=}\NormalTok{ np.zeros\_like(avg\_sens)}
\NormalTok{eff\_degree }\OperatorTok{=}\NormalTok{ np.zeros\_like(avg\_sens)}
\NormalTok{layer\_structures }\OperatorTok{=}\NormalTok{ []}

\ControlFlowTok{for}\NormalTok{ i, w }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(all\_hamming):}
\NormalTok{    layer }\OperatorTok{=}\NormalTok{ boolforge.utils.hamming\_weight\_to\_ncf\_layer\_structure(n, w)}
\NormalTok{    layer\_structures.append(layer)}
\NormalTok{    f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, layer\_structure}\OperatorTok{=}\NormalTok{layer)}
\NormalTok{    avg\_sens[i] }\OperatorTok{=}\NormalTok{ f.get\_average\_sensitivity(exact}\OperatorTok{=}\VariableTok{True}\NormalTok{, normalized}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{    can\_strength[i] }\OperatorTok{=}\NormalTok{ f.get\_canalizing\_strength()}
\NormalTok{    eff\_degree[i] }\OperatorTok{=}\NormalTok{ f.get\_effective\_degree()}

\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(}
\NormalTok{    \{}
        \StringTok{"Hamming weight"}\NormalTok{: all\_hamming,}
        \StringTok{"Absolute bias"}\NormalTok{: all\_abs\_bias,}
        \StringTok{"Layer structure"}\NormalTok{: }\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(}\BuiltInTok{str}\NormalTok{, layer\_structures)),}
        \StringTok{"Number of layers"}\NormalTok{: }\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(}\BuiltInTok{len}\NormalTok{, layer\_structures)),}
        \StringTok{"Average sensitivity"}\NormalTok{: avg\_sens,}
        \StringTok{"Canalizing strength"}\NormalTok{: np.}\BuiltInTok{round}\NormalTok{(can\_strength, }\DecValTok{4}\NormalTok{),}
        \StringTok{"Effective degree"}\NormalTok{: np.}\BuiltInTok{round}\NormalTok{(eff\_degree, }\DecValTok{4}\NormalTok{),}
\NormalTok{    \}}
\NormalTok{)}

\BuiltInTok{print}\NormalTok{(df.to\_string())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Hamming weight  Absolute bias Layer structure  Number of layers  Average sensitivity  Canalizing strength  Effective degree
0               1         0.0625             [5]                 1               0.3125               1.0000            1.1250
1               3         0.1875          [3, 2]                 2               0.6875               0.7705            1.3984
2               5         0.3125       [2, 1, 2]                 3               0.9375               0.6369            1.5938
3               7         0.4375          [2, 3]                 2               1.0625               0.5993            1.5833
4               9         0.5625       [1, 1, 3]                 3               1.1875               0.5033            1.7266
5              11         0.6875    [1, 1, 1, 2]                 4               1.3125               0.4657            1.8021
6              13         0.8125       [1, 2, 2]                 3               1.3125               0.4657            1.7708
7              15         0.9375          [1, 4]                 2               1.1875               0.5033            1.6094
\end{verbatim}

We notice that nested canalizing functions with higher absolute bias
tend to be more sensitive to input changes and also less canalizing.
However, the relationship between absolute bias and these other metrics
is far from monotonic. Further, we notice that there is a perfect
correlation between the average sensitivity of a nested canalizing
function and its canalizing strength, and a near perfect correlation
between average sensitivity and effective degree.

To investigate the non-monotonic behavior further, we can vary the
degree and create line plots that reveal a clear pattern, as shown in
Kadelka et al., Physica D, 2017.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{5}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ ns:}
\NormalTok{    all\_hamming\_weights }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, }\DecValTok{2} \OperatorTok{**}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{    all\_abs\_bias }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{abs}\NormalTok{(all\_hamming\_weights}\OperatorTok{/}\DecValTok{2}\OperatorTok{**}\NormalTok{n }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{)}
\NormalTok{    avg\_sens }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{**}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{2}\NormalTok{))}

    \ControlFlowTok{for}\NormalTok{ i, w }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(all\_hamming\_weights):}
\NormalTok{        layer }\OperatorTok{=}\NormalTok{ boolforge.utils.hamming\_weight\_to\_ncf\_layer\_structure(n, w)}
\NormalTok{        f }\OperatorTok{=}\NormalTok{ boolforge.random\_function(n, layer\_structure}\OperatorTok{=}\NormalTok{layer)}
\NormalTok{        avg\_sens[i] }\OperatorTok{=}\NormalTok{ f.get\_average\_sensitivity(exact}\OperatorTok{=}\VariableTok{True}\NormalTok{, normalized}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{    ax.plot(all\_abs\_bias, avg\_sens, }\StringTok{"x{-}{-}"}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f"n=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Absolute bias"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Average sensitivity"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial05_use_cases_random_function_generator_files/tutorial05_use_cases_random_function_generator_23_0.png}}
\caption{png}
\end{figure}

\subsection{Summary and outlook}\label{summary-and-outlook}

This tutorial illustrated how ensembles of Boolean functions generated
under explicit constraints reveal systematic relationships between
canalization, bias, redundancy, and sensitivity.

The key findings include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Canalization is rare in random functions but common in biology.
\item
  Canalizing strength and input redundancy both decrease with degree.
\item
  Functions with high absolute bias are more likely to be highly
  canalizing.
\item
  For NCFs, layer structure is uniquely determined by bias.
\item
  Average sensitivity varies systematically with layer structure.
\end{enumerate}

These relationships constrain the space of biologically plausible
functions and suggest evolutionary optimization for robustness.

We now move on to Boolean networks, where Boolean functions serve as
node update rules and give rise to collective dynamical behavior.

\section{Working with Boolean
Networks}\label{working-with-boolean-networks}

While previous tutorials focused on individual Boolean functions, this
tutorial introduces Boolean networks, which combine multiple Boolean
functions into a dynamical system.

\subsection{What you will learn}\label{what-you-will-learn-5}

In this tutorial you will learn how to:

\begin{itemize}
\tightlist
\item
  create Boolean networks,
\item
  compute basic properties of the wiring diagram,
\item
  compute basic properties of Boolean networks.
\end{itemize}

\subsection{0. Setup}\label{setup-5}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{Boolean network theory}\label{boolean-network-theory}

A Boolean network \(F = (f_1, \ldots, f_N)\) is a dynamical system
consisting of \(N\) Boolean update functions. Each node can be in one of
two states, 0 or 1, often interpreted as OFF/ON in biological contexts.

Under synchronous updating, all nodes update simultaneously, yielding a
deterministic state transition graph on \(\{0,1\}^N\).

Under asynchronous updating, only one node is updated at a time,
yielding a stochastic transition graph. BoolForge implements both
schemes.

Real biological networks are typically sparsely connected. The
\textbf{in-degree} of a node is the number of essential inputs of its
update function. The \textbf{wiring diagram} encodes which nodes
regulate which others.

Despite their simplicity, Boolean networks can:

\begin{itemize}
\tightlist
\item
  reproduce complex dynamics (oscillations, multistability),
\item
  predict gene knockout effects,
\item
  identify control strategies,
\item
  scale to genome-wide networks (1000s of nodes).
\end{itemize}

\subsection{Wiring diagrams}\label{wiring-diagrams}

We first construct wiring diagrams, which encode network structure
independently of specific Boolean functions.

Separating topology (I) from dynamics (F) allows:

\begin{itemize}
\tightlist
\item
  studying structural properties independent of specific Boolean rules,
\item
  swapping different rule sets on the same topology,
\item
  efficient storage (sparse I, local F vs dense full truth table).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Wiring diagram of a 3{-}node network}
\NormalTok{I }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{    [}\DecValTok{1}\NormalTok{],}
\NormalTok{]}

\NormalTok{W }\OperatorTok{=}\NormalTok{ boolforge.WiringDiagram(I}\OperatorTok{=}\NormalTok{I)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"W.N:"}\NormalTok{, W.N)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W.variables:"}\NormalTok{, W.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W.indegrees:"}\NormalTok{, W.indegrees)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W.outdegrees:"}\NormalTok{, W.outdegrees)}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ W.plot(show}\OperatorTok{=}\VariableTok{False}\NormalTok{)}\OperatorTok{;}
\NormalTok{fig}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
W.N: 3
W.variables: ['x0' 'x1' 'x2']
W.indegrees: [1 2 1]
W.outdegrees: [1 2 1]
\end{verbatim}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial06_boolean_networks_files/tutorial06_boolean_networks_4_1.png}}
\caption{png}
\end{figure}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial06_boolean_networks_files/tutorial06_boolean_networks_4_2.png}}
\caption{png}
\end{figure}

The wiring diagram above uses default variable names
\(x_0, \ldots, x_{N-1}\). The vectors \texttt{indegrees} and
\texttt{outdegrees} describe incoming and outgoing edges for each node.

\subsubsection{Example with constants and unequal
degrees}\label{example-with-constants-and-unequal-degrees}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{I }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{],}
\NormalTok{]}

\NormalTok{W }\OperatorTok{=}\NormalTok{ boolforge.WiringDiagram(I}\OperatorTok{=}\NormalTok{I)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"W.N:"}\NormalTok{, W.N)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W.variables:"}\NormalTok{, W.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W.indegrees:"}\NormalTok{, W.indegrees)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W.outdegrees:"}\NormalTok{, W.outdegrees)}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ W.plot(show}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{fig}

\end{Highlighting}
\end{Shaded}

\begin{verbatim}
W.N: 3
W.variables: ['x0' 'x1' 'x2']
W.indegrees: [0 2 1]
W.outdegrees: [2 0 1]
\end{verbatim}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial06_boolean_networks_files/tutorial06_boolean_networks_7_1.png}}
\caption{png}
\end{figure}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial06_boolean_networks_files/tutorial06_boolean_networks_7_2.png}}
\caption{png}
\end{figure}

This wiring diagram encodes a \textbf{feed-forward loop}, one of the
most common \emph{network motifs} in transcriptional networks. It can:

\begin{itemize}
\tightlist
\item
  filter transient signals (coherent FFL with AND gate),
\item
  accelerate response (incoherent FFL),
\end{itemize}

See Mangan \& Alon, PNAS, 2003 for a detailed analysis.
\texttt{BoolForge} enables the identification of all feed-forward loops:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"W.get\_ffls()"}\NormalTok{, W.get\_ffls())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
W.get_ffls() {'FFLs': [[0, 2, 1]]}
\end{verbatim}

This tells us that \texttt{W} contains one FFL, in which \(x_0\)
regulates both \(x_1\) and \(x_2\), while \(x_1\) is also regulated by
\(x_2\).

\texttt{BoolForge} can also identify all feedback loops. For this, we
consider another wiring diagram:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{I2 }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{1}\NormalTok{],}
\NormalTok{]}

\NormalTok{W2 }\OperatorTok{=}\NormalTok{ boolforge.WiringDiagram(I}\OperatorTok{=}\NormalTok{I2)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ W2.plot(show}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{fig}


\BuiltInTok{print}\NormalTok{(}\StringTok{"W2.get\_fbls()"}\NormalTok{, W2.get\_fbls())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
W2.get_fbls() {'FBLs': [[0, 1, 2], [0, 1]]}
\end{verbatim}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial06_boolean_networks_files/tutorial06_boolean_networks_11_1.png}}
\caption{png}
\end{figure}

The function \texttt{.get\_fbls()} identifies all simple cycles in the
wiring diagram. In this case, there exists a 2-cycle
\(x_0 \leftrightarrow x_1\) and a 3-cycle
\(x_0 \to x_1 \to x_2 \to x_0\).

\subsection{Creating Boolean networks}\label{creating-boolean-networks}

To create a Boolean network, we must specify:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A wiring diagram \texttt{I}, describing who regulates whom.
\item
  A list \texttt{F} of Boolean update functions (or truth tables), one
  per node.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{I }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{    [}\DecValTok{1}\NormalTok{],}
\NormalTok{]}

\NormalTok{F }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{]}

\NormalTok{bn }\OperatorTok{=}\NormalTok{ boolforge.BooleanNetwork(F}\OperatorTok{=}\NormalTok{F, I}\OperatorTok{=}\NormalTok{I)}

\NormalTok{bn.to\_truth\_table()}
\end{Highlighting}
\end{Shaded}

x0(t)

x1(t)

x2(t)

x0(t+1)

x1(t+1)

x2(t+1)

0

0

0

0

0

0

0

1

0

0

1

0

1

0

2

0

1

0

1

0

1

3

0

1

1

1

1

1

4

1

0

0

0

1

0

5

1

0

1

0

1

0

6

1

1

0

1

1

1

7

1

1

1

1

1

1

The full truth table of a Boolean network has size \(N \times 2^N\) and
therefore grows exponentially with the number of nodes.\\
In practice, however, \texttt{BoolForge} never stores this object
explicitly. Instead, a Boolean network is represented internally by its
wiring diagram \texttt{I} and the list of update functions \texttt{F},
which is far more memory-efficient -- especially for sparse networks
with few regulators per node.

When a Boolean network is constructed from \texttt{F} and \texttt{I},
\texttt{BoolForge} automatically performs a series of consistency checks
to guard against common modeling errors. For example, it verifies that
each update function has the correct length, namely \(2^n\), where \(n\)
is the number of regulators of the corresponding node as specified in
\texttt{I}. If any of these checks fail, an informative error is raised
immediately, helping ensure that the resulting network is well-defined.

\subsubsection{Creating networks from
strings}\label{creating-networks-from-strings}

Alternatively, Boolean networks can be specified using a human-readable
string representation, where each line defines the update rule of one
node. This format closely mirrors the way Boolean models are written in
the literature and is often more convenient than manually specifying
wiring diagrams and truth tables.

In the example below, each line has the form
\(x_i = f_i(\text{regulators of } x_i),\) where Boolean operators such
as \texttt{AND}, \texttt{OR}, and \texttt{NOT} can be used to define the
update functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OperatorTok{=} \StringTok{"""}
\StringTok{x = y}
\StringTok{y = x OR z}
\StringTok{z = y}
\StringTok{"""}

\NormalTok{bn\_str }\OperatorTok{=}\NormalTok{ boolforge.BooleanNetwork.from\_string(string, separator}\OperatorTok{=}\StringTok{"="}\NormalTok{)}
\NormalTok{bn\_str.to\_truth\_table()}
\end{Highlighting}
\end{Shaded}

x(t)

y(t)

z(t)

x(t+1)

y(t+1)

z(t+1)

0

0

0

0

0

0

0

1

0

0

1

0

1

0

2

0

1

0

1

0

1

3

0

1

1

1

1

1

4

1

0

0

0

1

0

5

1

0

1

0

1

0

6

1

1

0

1

1

1

7

1

1

1

1

1

1

Here, the update rule \texttt{x\ =\ y} specifies that node \texttt{x}
copies the state of \texttt{y}, while \texttt{y\ =\ x\ OR\ z} indicates
that node \texttt{y} is regulated by both \texttt{x} and \texttt{z}.
From this symbolic description, \texttt{BoolForge} automatically:

\begin{itemize}
\tightlist
\item
  extracts the wiring diagram,
\item
  determines the regulators of each node,
\item
  constructs the corresponding Boolean update functions.
\end{itemize}

Internally, the string representation is converted into the same
\texttt{(F,\ I)} representation used throughout the package. As a
result, Boolean networks created from strings behave identically to
those created explicitly from wiring diagrams and truth tables.

This interface is particularly useful for loading Boolean network models
from external sources, such as \texttt{.bnet} files, or for quickly
prototyping models in an interactive setting.

\subsubsection{Interoperability with
CANA}\label{interoperability-with-cana}

\texttt{BoolForge} provides native interoperability with the
\texttt{CANA} package for the analysis of Boolean functions and Boolean
networks. Existing \texttt{BoolForge} networks can be converted into
CANA objects and back without loss of information.

In the example below, we convert a \texttt{BoolForge} Boolean network
into its CANA representation using \texttt{to\_cana()}, and then
reconstruct a new \texttt{BoolForge} Boolean network from that CANA
object.

The final assertion verifies that this round-trip conversion preserves:

\begin{itemize}
\tightlist
\item
  the Boolean update functions,
\item
  the wiring diagram,
\item
  and the variable names.
\end{itemize}

This guarantees that \texttt{BoolForge} and CANA can be used
interchangeably within a workflow, allowing users to leverage CANA's
analytical tools while continuing to build and manipulate models using
\texttt{BoolForge}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cana\_bn }\OperatorTok{=}\NormalTok{ bn.to\_cana()}
\NormalTok{bn\_from\_cana }\OperatorTok{=}\NormalTok{ boolforge.BooleanNetwork.from\_cana(cana\_bn)}

\ControlFlowTok{assert}\NormalTok{ (}
\NormalTok{    np.}\BuiltInTok{all}\NormalTok{([np.}\BuiltInTok{all}\NormalTok{(bn.F[i].f }\OperatorTok{==}\NormalTok{ bn\_from\_cana.F[i].f) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(bn.N)])}
    \KeywordTok{and}\NormalTok{ np.}\BuiltInTok{all}\NormalTok{([np.}\BuiltInTok{all}\NormalTok{(bn.I[i] }\OperatorTok{==}\NormalTok{ bn\_from\_cana.I[i]) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(bn.N)])}
    \KeywordTok{and}\NormalTok{ np.}\BuiltInTok{all}\NormalTok{(bn.variables }\OperatorTok{==}\NormalTok{ bn\_from\_cana.variables)}
\NormalTok{), }\StringTok{"BooleanNetwork CANA conversion failed"}
\end{Highlighting}
\end{Shaded}

\subsection{Types of nodes in Boolean
networks}\label{types-of-nodes-in-boolean-networks}

Nodes in a Boolean network can be classified as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Constant nodes}\\
  Nodes with constant update functions (always 0 or always 1). These
  nodes act as parameters and are removed internally, with their values
  propagated through the network.
\item
  \textbf{Identity nodes}\\
  Nodes whose update function is the identity, i.e., \(f(x_i) = x_i.\)
  Their value is determined by the initial condition and remains
  constant over time. Identity nodes are retained as part of the Boolean
  network state. They may be viewed as nodes with a self-loop and no
  other incoming edges.
\item
  \textbf{Regulated nodes}\\
  Nodes whose update functions depend on one or more other nodes.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{F }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{],  }\CommentTok{\# regulated}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{],  }\CommentTok{\# regulated}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{],        }\CommentTok{\# identity}
\NormalTok{    [}\DecValTok{0}\NormalTok{],           }\CommentTok{\# constant}
\NormalTok{]}

\NormalTok{I }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{],        }\CommentTok{\# regulated}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{],        }\CommentTok{\# regulated}
\NormalTok{    [}\DecValTok{2}\NormalTok{],           }\CommentTok{\# identity}
\NormalTok{    [],            }\CommentTok{\# constant}
\NormalTok{]}

\NormalTok{bn }\OperatorTok{=}\NormalTok{ boolforge.BooleanNetwork(F, I)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.variables:"}\NormalTok{, bn.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.constants:"}\NormalTok{, bn.constants)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.F:"}\NormalTok{, bn.F)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.I:"}\NormalTok{, bn.I)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
bn.variables: ['x0' 'x1' 'x2']
bn.constants: {'x3': {'value': 0, 'regulatedNodes': ['x1']}}
bn.F: [BooleanFunction(f=[0, 0, 0, 1]), BooleanFunction(f=[0, 1]), BooleanFunction(f=[0, 1])]
bn.I: [array([1, 2]), array([0]), array([2])]
\end{verbatim}

The constant node is removed, and its value is propagated into
downstream update functions.

If we now change the value of the constant node from 0 to 1, the network
is constructed in the same way, and the constant value 1 is substituted
directly into all downstream update functions, before removal of the
constant node.

As a result, the Boolean update functions of downstream nodes may
simplify, potentially reducing the number of regulators or changing the
logical form of the function. This illustrates how constant nodes act as
parameters whose values influence the effective dynamics of the network.

Importantly, this simplification is performed symbolically at
construction time and does not depend on the dynamical evolution of the
network.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{F }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{1}\NormalTok{],}
\NormalTok{]}

\NormalTok{I }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{],}
\NormalTok{    [}\DecValTok{2}\NormalTok{],}
\NormalTok{    [],}
\NormalTok{]}

\NormalTok{bn }\OperatorTok{=}\NormalTok{ boolforge.BooleanNetwork(F, I)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.F:"}\NormalTok{, bn.F)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.I:"}\NormalTok{, bn.I)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.variables:"}\NormalTok{, bn.variables)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
bn.F: [BooleanFunction(f=[0, 0, 0, 1]), BooleanFunction(f=[1, 1]), BooleanFunction(f=[0, 1])]
bn.I: [array([1, 2]), array([0]), array([2])]
bn.variables: ['x0' 'x1' 'x2']
\end{verbatim}

Although \(x_1\) becomes fixed at 1 after one update, it is not treated
as a constant node. In \texttt{BoolForge}, constant nodes are identified
by their update functions (always 0 or always 1), not by their long-term
dynamical behavior. Since \(x_1 = 0\) remains a valid initial condition,
the node is retained as part of the network state.

\subsection{Boolean network
properties}\label{boolean-network-properties}

The class \texttt{BooleanNetwork} inherits basic structural properties
and methods from \texttt{WiringDiagram}. In particular, all
graph-theoretic attributes of the wiring diagram -- such as the number
of nodes, in-degrees, and out-degrees -- are directly accessible on a
Boolean network object.

Moreover, \texttt{BooleanNetwork} inherits visualization utilities from
\texttt{WiringDiagram}, including methods for plotting the wiring
diagram and its modular structure, using \texttt{.plot()}. This allows
users to inspect the topology of a Boolean network independently of the
specific update functions.

Beyond these inherited features, \texttt{BooleanNetwork} provides a rich
collection of additional methods for analyzing the dynamics, structure,
and control properties of Boolean networks. These include functionality
for:

\begin{itemize}
\tightlist
\item
  computing fixed points and attractors,
\item
  analyzing transient dynamics and state transition graphs,
\item
  studying robustness and sensitivity to perturbations,
\item
  performing node and edge interventions.
\end{itemize}

Many of these methods will be introduced and discussed in detail in the
following tutorials. Here, we focus only on a few basic and commonly
used properties.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.N:"}\NormalTok{, bn.N)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.indegrees:"}\NormalTok{, bn.indegrees)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.outdegrees:"}\NormalTok{, bn.outdegrees)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.variables:"}\NormalTok{, bn.variables)}

\NormalTok{bn.plot()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
bn.N: 3
bn.indegrees: [2 1 1]
bn.outdegrees: [1 1 2]
bn.variables: ['x0' 'x1' 'x2']
\end{verbatim}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial06_boolean_networks_files/tutorial06_boolean_networks_27_1.png}}
\caption{png}
\end{figure}

\subsection{Outlook}\label{outlook}

In the remaining tutorials, we build on this foundation to study the
dynamical behavior of Boolean networks, including attractors, basins of
attraction, and stability under perturbations.

\section{Dynamics of Boolean
Networks}\label{dynamics-of-boolean-networks}

In this tutorial, we study the \emph{dynamics} of Boolean networks.
Building on the construction and structural analysis from previous
tutorials, we now focus on how Boolean networks evolve over time and how
their long-term behavior can be characterized.

You will learn how to:

\begin{itemize}
\tightlist
\item
  simulate Boolean network dynamics under different updating schemes,
\item
  compute and classify attractors,
\item
  analyze basins of attraction,
\item
  relate network structure to dynamical behavior.
\end{itemize}

\subsection{Setup}\label{setup-6}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{State space of a Boolean
network}\label{state-space-of-a-boolean-network}

A Boolean network with \(N\) nodes defines a dynamical system on the
discrete state space \(\{0,1\}^N\).

Each state is a binary vector

\[
\mathbf{x} = (x_0, \ldots, x_{N-1}) \in \{0,1\}^N,
\]

where \(x_i\) denotes the state of node \(i\).

We use a small Boolean network as a running example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OperatorTok{=} \StringTok{"""}
\StringTok{x = y}
\StringTok{y = x OR z}
\StringTok{z = y}
\StringTok{"""}

\NormalTok{bn }\OperatorTok{=}\NormalTok{ boolforge.BooleanNetwork.from\_string(string, separator}\OperatorTok{=}\StringTok{"="}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Variables:"}\NormalTok{, bn.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"N:"}\NormalTok{, bn.N)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.I:"}\NormalTok{, bn.I)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"bn.F:"}\NormalTok{, bn.F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Variables: ['x' 'y' 'z']
N: 3
bn.I: [array([1]), array([0, 2]), array([1])]
bn.F: [BooleanFunction(f=[0, 1]), BooleanFunction(f=[0, 1, 1, 1]), BooleanFunction(f=[0, 1])]
\end{verbatim}

All state vectors follow the variable order given by
\texttt{bn.variables}. For small networks, we can enumerate all \(2^N\)
states explicitly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_states }\OperatorTok{=}\NormalTok{ boolforge.get\_left\_side\_of\_truth\_table(bn.N)}
\NormalTok{pd.DataFrame(all\_states, columns}\OperatorTok{=}\NormalTok{bn.variables)}
\end{Highlighting}
\end{Shaded}

x

y

z

0

0

0

0

1

0

0

1

2

0

1

0

3

0

1

1

4

1

0

0

5

1

0

1

6

1

1

0

7

1

1

1

\subsection{Dynamics of synchronous Boolean
networks}\label{dynamics-of-synchronous-boolean-networks}

Under \emph{synchronous updating}, all nodes are updated simultaneously,
defining a deterministic update map

\[
\mathbf{x}(t+1) = F(\mathbf{x}(t)).
\]

\subsubsection{Exact computation}\label{exact-computation}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ state }\KeywordTok{in}\NormalTok{ all\_states:}
    \BuiltInTok{print}\NormalTok{(state, }\StringTok{"{-}{-}\textgreater{}"}\NormalTok{, bn.update\_network\_synchronously(state))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0 0 0] --> [0 0 0]
[0 0 1] --> [0 1 0]
[0 1 0] --> [1 0 1]
[0 1 1] --> [1 1 1]
[1 0 0] --> [0 1 0]
[1 0 1] --> [0 1 0]
[1 1 0] --> [1 1 1]
[1 1 1] --> [1 1 1]
\end{verbatim}

This output matches the synchronous truth table representation:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(bn.to\_truth\_table().to\_string())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   x(t)  y(t)  z(t)  x(t+1)  y(t+1)  z(t+1)
0     0     0     0       0       0       0
1     0     0     1       0       1       0
2     0     1     0       1       0       1
3     0     1     1       1       1       1
4     1     0     0       0       1       0
5     1     0     1       0       1       0
6     1     1     0       1       1       1
7     1     1     1       1       1       1
\end{verbatim}

Each state has exactly one successor, so the dynamics consist of
transient trajectories leading into \emph{attractors} (steady states or
cycles).

In this example, the network has:

\begin{itemize}
\tightlist
\item
  two steady states: \((0,0,0)\) and \((1,1,1)\),
\item
  one cyclic attractor of length 2: \((0,1,0) \leftrightarrow (1,0,1)\).
\end{itemize}

\subsubsection{Exhaustive attractor
computation}\label{exhaustive-attractor-computation}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dict\_dynamics }\OperatorTok{=}\NormalTok{ bn.get\_attractors\_synchronous\_exact()}
\NormalTok{dict\_dynamics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'Attractors': [[0], [2, 5], [7]],
 'NumberOfAttractors': 3,
 'BasinSizes': array([0.125, 0.5  , 0.375]),
 'AttractorID': array([0, 1, 1, 2, 1, 1, 2, 2], dtype=int32),
 'STG': array([0, 2, 5, 7, 2, 2, 7, 7])}
\end{verbatim}

The returned dictionary contains:

\begin{itemize}
\tightlist
\item
  \texttt{STG}: the synchronous state transition graph,
\item
  \texttt{NumberOfAttractors},
\item
  \texttt{Attractors},
\item
  \texttt{AttractorDict},
\item
  \texttt{BasinSizes}.
\end{itemize}

The state transition graph can be decoded as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ state }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{2} \OperatorTok{**}\NormalTok{ bn.N):}
\NormalTok{    next\_state }\OperatorTok{=}\NormalTok{ dict\_dynamics[}\StringTok{"STG"}\NormalTok{][state]}
    \BuiltInTok{print}\NormalTok{(}
\NormalTok{        state,}
        \StringTok{"="}\NormalTok{,}
\NormalTok{        boolforge.dec2bin(state, bn.N),}
        \StringTok{"{-}{-}\textgreater{}"}\NormalTok{,}
\NormalTok{        next\_state,}
        \StringTok{"="}\NormalTok{,}
\NormalTok{        boolforge.dec2bin(next\_state, bn.N),}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0 = [0, 0, 0] --> 0 = [0, 0, 0]
1 = [0, 0, 1] --> 2 = [0, 1, 0]
2 = [0, 1, 0] --> 5 = [1, 0, 1]
3 = [0, 1, 1] --> 7 = [1, 1, 1]
4 = [1, 0, 0] --> 2 = [0, 1, 0]
5 = [1, 0, 1] --> 2 = [0, 1, 0]
6 = [1, 1, 0] --> 7 = [1, 1, 1]
7 = [1, 1, 1] --> 7 = [1, 1, 1]
\end{verbatim}

Attractors can be printed in binary representation:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ attractor }\KeywordTok{in}\NormalTok{ dict\_dynamics[}\StringTok{"Attractors"}\NormalTok{]:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Attractor of length }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(attractor)}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ state }\KeywordTok{in}\NormalTok{ attractor:}
        \BuiltInTok{print}\NormalTok{(state, boolforge.dec2bin(state, bn.N))}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Attractor of length 1:
0 [0, 0, 0]

Attractor of length 2:
2 [0, 1, 0]
5 [1, 0, 1]

Attractor of length 1:
7 [1, 1, 1]
\end{verbatim}

Basin sizes count how many states flow into each attractor. They always
sum to \(2^N\).

\subsubsection{Monte Carlo simulation}\label{monte-carlo-simulation}

For larger networks, exhaustive enumeration is infeasible. Monte Carlo
simulation approximates the attractor landscape.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dict\_dynamics }\OperatorTok{=}\NormalTok{ bn.get\_attractors\_synchronous(n\_simulations}\OperatorTok{=}\DecValTok{100}\NormalTok{)}
\NormalTok{dict\_dynamics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'Attractors': [[2, 5], [7], [0]],
 'NumberOfAttractors': 3,
 'BasinSizes': [50, 38, 12],
 'AttractorDict': {2: 0, 5: 0, 7: 1, 1: 0, 4: 0, 3: 1, 0: 2, 6: 1},
 'InitialSamplePoints': [2,
  7,
  5,
  1,
  7,
  5,
  1,
  1,
  4,
  3,
  0,
  2,
  7,
  4,
  5,
  4,
  5,
  1,
  0,
  4,
  2,
  4,
  4,
  6,
  0,
  6,
  3,
  4,
  6,
  4,
  7,
  3,
  3,
  6,
  6,
  6,
  6,
  7,
  1,
  6,
  3,
  0,
  2,
  0,
  4,
  7,
  6,
  3,
  3,
  5,
  5,
  5,
  6,
  4,
  1,
  2,
  1,
  4,
  0,
  0,
  5,
  6,
  0,
  6,
  2,
  1,
  5,
  7,
  7,
  0,
  7,
  7,
  1,
  6,
  7,
  7,
  0,
  1,
  2,
  0,
  2,
  0,
  2,
  1,
  1,
  7,
  1,
  2,
  2,
  1,
  7,
  7,
  5,
  7,
  2,
  2,
  3,
  1,
  7,
  1],
 'STG': {2: 5, 7: 7, 5: 2, 1: 2, 4: 2, 3: 7, 0: 0, 6: 7},
 'NumberOfTimeouts': 0}
\end{verbatim}

The simulation returns additional information:

\begin{itemize}
\tightlist
\item
  sampled initial states,
\item
  the number of timeouts (trajectories not reaching an attractor in
  time).
\end{itemize}

If an attractor has relative basin size \(q\), the probability that it
is found after \(m\) random initializations is \(1 - (1-q)^m\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qs }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.0001}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{]}
\NormalTok{ms }\OperatorTok{=}\NormalTok{ np.logspace(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1000}\NormalTok{)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\ControlFlowTok{for}\NormalTok{ q }\KeywordTok{in}\NormalTok{ qs:}
\NormalTok{    ax.semilogx(ms, }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ q) }\OperatorTok{**}\NormalTok{ ms, label}\OperatorTok{=}\BuiltInTok{str}\NormalTok{(q))}

\NormalTok{ax.legend(title}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{q}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"number of initial states ($m$)"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"probability attractor is found"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial07_boolean_network_dynamics_files/tutorial07_boolean_network_dynamics_22_0.png}}
\caption{png}
\end{figure}

\subsection{Dynamics of asynchronous Boolean
networks}\label{dynamics-of-asynchronous-boolean-networks}

Synchronous updating is computationally convenient but biologically
unrealistic. Asynchronous updating assumes that only one node is updated
at a time.

\subsubsection{Steady states under general asynchronous
update}\label{steady-states-under-general-asynchronous-update}

BoolForge can compute steady states under general asynchronous updating.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dict\_dynamics }\OperatorTok{=}\NormalTok{ bn.get\_steady\_states\_asynchronous\_exact()}
\NormalTok{dict\_dynamics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'SteadyStates': [0, 7],
 'NumberOfSteadyStates': 2,
 'BasinSizes': array([0.33333333, 0.66666667]),
 'STGAsynchronous': {0: {0: np.float64(1.0)},
  1: {1: np.float64(0.3333333333333333),
   3: np.float64(0.3333333333333333),
   0: np.float64(0.3333333333333333)},
  2: {6: np.float64(0.3333333333333333),
   0: np.float64(0.3333333333333333),
   3: np.float64(0.3333333333333333)},
  3: {7: np.float64(0.3333333333333333), 3: np.float64(0.6666666666666666)},
  4: {0: np.float64(0.3333333333333333),
   6: np.float64(0.3333333333333333),
   4: np.float64(0.3333333333333333)},
  5: {1: np.float64(0.3333333333333333),
   7: np.float64(0.3333333333333333),
   4: np.float64(0.3333333333333333)},
  6: {6: np.float64(0.6666666666666666), 7: np.float64(0.3333333333333333)},
  7: {7: np.float64(1.0)}},
 'FinalTransitionProbabilities': array([[1.        , 0.        ],
        [0.5       , 0.5       ],
        [0.33333333, 0.66666667],
        [0.        , 1.        ],
        [0.5       , 0.5       ],
        [0.33333333, 0.66666667],
        [0.        , 1.        ],
        [0.        , 1.        ]])}
\end{verbatim}

This reveals the same two steady states as in the synchronous case. In
addition, the full asynchronous transition graph and absorption
probabilities are returned.

BoolForge currently does not detect complex cyclic attractors under
asynchronous updating; for those, specialized tools such as
\texttt{pystablemotifs} are recommended.

\subsubsection{Monte Carlo
approximation}\label{monte-carlo-approximation}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dict\_dynamics }\OperatorTok{=}\NormalTok{ bn.get\_steady\_states\_asynchronous(n\_simulations}\OperatorTok{=}\DecValTok{500}\NormalTok{)}
\NormalTok{dict\_dynamics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'SteadyStates': [7, 0],
 'NumberOfSteadyStates': 2,
 'BasinSizes': [344, 156],
 'STGAsynchronous': {(6, 1): 6,
  (6, 0): 6,
  (6, 2): 7,
  (7, 1): 7,
  (7, 0): 7,
  (7, 2): 7,
  (4, 0): 0,
  (0, 1): 0,
  (0, 2): 0,
  (0, 0): 0,
  (4, 1): 6,
  (5, 2): 4,
  (2, 2): 3,
  (3, 0): 7,
  (3, 1): 3,
  (3, 2): 3,
  (4, 2): 4,
  (1, 2): 0,
  (1, 0): 1,
  (2, 0): 6,
  (1, 1): 3,
  (5, 0): 1,
  (5, 1): 7,
  (2, 1): 0},
 'InitialSamplePoints': [6,
  4,
  4,
  0,
  5,
  2,
  3,
  2,
  7,
  4,
  3,
  0,
  7,
  1,
  4,
  1,
  0,
  7,
  7,
  1,
  1,
  6,
  2,
  4,
  1,
  1,
  2,
  1,
  4,
  3,
  2,
  7,
  7,
  5,
  5,
  3,
  0,
  0,
  7,
  1,
  3,
  3,
  6,
  3,
  1,
  4,
  2,
  4,
  6,
  2,
  2,
  0,
  5,
  0,
  2,
  5,
  2,
  3,
  1,
  0,
  4,
  1,
  6,
  4,
  3,
  3,
  7,
  4,
  1,
  6,
  5,
  7,
  2,
  5,
  6,
  4,
  7,
  0,
  3,
  2,
  6,
  4,
  6,
  4,
  5,
  5,
  6,
  2,
  1,
  0,
  2,
  2,
  6,
  4,
  2,
  2,
  2,
  0,
  7,
  7,
  6,
  7,
  3,
  2,
  4,
  0,
  4,
  3,
  1,
  1,
  4,
  5,
  6,
  4,
  5,
  5,
  2,
  3,
  3,
  5,
  4,
  5,
  2,
  3,
  0,
  0,
  1,
  1,
  6,
  4,
  7,
  3,
  7,
  6,
  0,
  0,
  5,
  1,
  1,
  4,
  5,
  1,
  4,
  6,
  5,
  3,
  6,
  0,
  4,
  0,
  7,
  4,
  0,
  5,
  7,
  0,
  6,
  0,
  3,
  1,
  6,
  3,
  2,
  1,
  3,
  3,
  4,
  7,
  0,
  0,
  6,
  7,
  5,
  7,
  5,
  2,
  3,
  7,
  7,
  5,
  3,
  0,
  5,
  5,
  7,
  6,
  0,
  2,
  4,
  2,
  2,
  0,
  7,
  0,
  5,
  3,
  6,
  2,
  5,
  2,
  2,
  7,
  2,
  6,
  3,
  5,
  6,
  7,
  6,
  3,
  7,
  4,
  6,
  7,
  6,
  3,
  4,
  1,
  3,
  3,
  2,
  7,
  7,
  3,
  7,
  2,
  1,
  1,
  3,
  6,
  3,
  1,
  6,
  0,
  4,
  0,
  4,
  6,
  5,
  2,
  1,
  6,
  2,
  2,
  0,
  7,
  5,
  7,
  1,
  2,
  3,
  6,
  2,
  1,
  7,
  3,
  2,
  2,
  2,
  6,
  7,
  4,
  4,
  3,
  2,
  7,
  1,
  3,
  0,
  5,
  3,
  5,
  7,
  6,
  0,
  7,
  1,
  6,
  7,
  3,
  0,
  0,
  0,
  7,
  1,
  0,
  5,
  1,
  2,
  5,
  2,
  3,
  4,
  3,
  6,
  4,
  5,
  3,
  3,
  0,
  2,
  4,
  7,
  2,
  1,
  4,
  0,
  0,
  3,
  4,
  5,
  3,
  1,
  6,
  2,
  5,
  0,
  7,
  6,
  6,
  2,
  1,
  2,
  1,
  2,
  3,
  0,
  4,
  4,
  3,
  7,
  0,
  1,
  6,
  2,
  7,
  7,
  3,
  6,
  0,
  2,
  1,
  1,
  5,
  6,
  0,
  5,
  4,
  7,
  2,
  7,
  5,
  6,
  0,
  6,
  6,
  4,
  5,
  7,
  6,
  5,
  5,
  2,
  2,
  5,
  3,
  2,
  1,
  3,
  7,
  2,
  3,
  2,
  0,
  7,
  6,
  3,
  0,
  1,
  4,
  0,
  5,
  7,
  0,
  2,
  2,
  2,
  6,
  4,
  0,
  1,
  4,
  1,
  3,
  1,
  7,
  0,
  6,
  6,
  1,
  4,
  6,
  5,
  5,
  7,
  1,
  1,
  5,
  5,
  3,
  3,
  6,
  5,
  5,
  3,
  5,
  5,
  2,
  6,
  0,
  5,
  4,
  7,
  3,
  3,
  2,
  7,
  6,
  5,
  5,
  4,
  6,
  7,
  5,
  5,
  5,
  3,
  3,
  7,
  7,
  4,
  6,
  7,
  7,
  0,
  5,
  7,
  1,
  0,
  7,
  2,
  0,
  0,
  7,
  3,
  6,
  3,
  2,
  6,
  4,
  7,
  4,
  0,
  5,
  0,
  1,
  2,
  0,
  0,
  1,
  6,
  5,
  6,
  5,
  4,
  2,
  5,
  3,
  7,
  3,
  1,
  6,
  5,
  7,
  5,
  7,
  5,
  2,
  6,
  3,
  0,
  1,
  4,
  7,
  0,
  2,
  4,
  4,
  5,
  1]}
\end{verbatim}

The simulation provides:

\begin{itemize}
\tightlist
\item
  a lower bound on the number of steady states,
\item
  approximate basin size distributions,
\item
  samples of the asynchronous state transition graph.
\end{itemize}

\subsubsection{Sampling from a fixed initial
condition}\label{sampling-from-a-fixed-initial-condition}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dict\_dynamics }\OperatorTok{=}\NormalTok{ bn.get\_steady\_states\_asynchronous\_given\_one\_initial\_condition(}
\NormalTok{    initial\_condition}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], n\_simulations}\OperatorTok{=}\DecValTok{500}
\NormalTok{)}
\NormalTok{dict\_dynamics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'SteadyStates': [0, 7, 9, 2],
 'NumberOfSteadyStates': 4,
 'BasinSizes': [246, 132, 36, 86],
 'TransientTimes': [[1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1],
  [2,
   3,
   2,
   2,
   2,
   3,
   3,
   2,
   2,
   2,
   2,
   2,
   3,
   2,
   2,
   2,
   3,
   2,
   3,
   2,
   2,
   3,
   3,
   2,
   3,
   2,
   2,
   3,
   2,
   2,
   2,
   3,
   3,
   2,
   3,
   2,
   3,
   3,
   2,
   3,
   2,
   3,
   2,
   2,
   3,
   2,
   3,
   3,
   2,
   2,
   3,
   3,
   2,
   2,
   3,
   3,
   3,
   2,
   3,
   3,
   2,
   3,
   3,
   3,
   2,
   2,
   3,
   2,
   2,
   2,
   2,
   2,
   2,
   3,
   3,
   2,
   3,
   3,
   3,
   3,
   2,
   2,
   3,
   2,
   3,
   2,
   2,
   3,
   3,
   2,
   2,
   3,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   3,
   2,
   3,
   2,
   2,
   2,
   2,
   3,
   2,
   2,
   2,
   2,
   3,
   3,
   2,
   3,
   3,
   2,
   2,
   2,
   2,
   2,
   2,
   3,
   3,
   2,
   2,
   2,
   3,
   3,
   2,
   3],
  [3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3,
   3],
  [2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2,
   2]],
 'STGAsynchronous': {(1, 2): 0,
  (0, 2): 0,
  (0, 1): 0,
  (0, 0): 0,
  (1, 1): 3,
  (3, 0): 7,
  (7, 2): 7,
  (7, 1): 7,
  (7, 0): 7,
  (1, 0): 1,
  (3, 1): 5,
  (5, 2): 5,
  (5, 0): 9,
  (9, 0): 9,
  (9, 2): 9,
  (9, 1): 9,
  (3, 2): 2,
  (2, 0): 2,
  (2, 1): 2,
  (2, 2): 2,
  (5, 1): 7},
 'UpdateQueues': [[1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 3, 5, 9],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 2],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 5, 9],
  [1, 3, 2],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 3, 7],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 9],
  [1, 3, 2],
  [1, 3, 5, 9],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 5, 9],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 3, 2],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 5, 7],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 3, 2],
  [1, 0],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 2],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 3, 2],
  [1, 3, 2],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 2],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 5, 9],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 3, 7],
  [1, 3, 2],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 9],
  [1, 3, 7],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 5, 9],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 7],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 3, 5, 7],
  [1, 3, 7],
  [1, 0],
  [1, 0],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 7],
  [1, 0],
  [1, 3, 5, 9],
  [1, 0],
  [1, 3, 2],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 2],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 0],
  [1, 3, 5, 7],
  [1, 0],
  [1, 3, 2],
  [1, 3, 2],
  [1, 3, 7],
  [1, 3, 5, 7],
  [1, 3, 2],
  [1, 0],
  [1, 0]]}
\end{verbatim}

\subsection{Summary and outlook}\label{summary-and-outlook-1}

In this tutorial you learned how to:

\begin{itemize}
\tightlist
\item
  simulate Boolean network dynamics,
\item
  compute synchronous attractors exactly and approximately,
\item
  analyze basin sizes,
\item
  compute steady states under asynchronous updating.
\end{itemize}

This concludes the function- and network-level analysis. Subsequent work
focuses on large-scale dynamical analysis, perturbations, and stability
in Boolean network models.

\section{Perturbation and sensitivity analysis of Boolean
networks}\label{perturbation-and-sensitivity-analysis-of-boolean-networks}

In this tutorial, we study how Boolean networks respond to
perturbations. Rather than implementing perturbations manually, we
leverage BoolForge's built-in robustness and sensitivity measures.

You will learn how to: - quantify robustness and fragility of Boolean
networks under synchronous update, - interpret basin-level and
attractor-level robustness measures, - compare exact and approximate
robustness computations, and - compute Derrida values as a measure of
dynamical sensitivity.

These tools allow us to assess dynamical stability and resilience of
Boolean network models in a principled and computationally efficient
way.

\subsection{Setup}\label{setup-7}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ boolforge}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{A running example Boolean
network}\label{a-running-example-boolean-network}

We reuse the small Boolean network from the previous tutorial.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{string }\OperatorTok{=} \StringTok{"""}
\StringTok{x = y}
\StringTok{y = x OR z}
\StringTok{z = y}
\StringTok{"""}

\NormalTok{bn }\OperatorTok{=}\NormalTok{ boolforge.BooleanNetwork.from\_string(string, separator}\OperatorTok{=}\StringTok{"="}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Variables:"}\NormalTok{, bn.variables)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Number of nodes:"}\NormalTok{, bn.N)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Variables: ['x' 'y' 'z']
Number of nodes: 3
\end{verbatim}

\subsection{Exact attractors and robustness
measures}\label{exact-attractors-and-robustness-measures}

BoolForge provides a single method that computes: - all attractors, -
basin sizes, - overall network coherence and fragility, - basin-level
coherence and fragility, and - attractor-level coherence and fragility.

These quantities are defined via systematic single-bit perturbations in
the Boolean hypercube and can be computed \emph{exactly} for small
networks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results\_exact }\OperatorTok{=}\NormalTok{ bn.get\_attractors\_and\_robustness\_synchronous\_exact()}
\NormalTok{results\_exact.keys()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
dict_keys(['Attractors', 'NumberOfAttractors', 'BasinSizes', 'AttractorID', 'Coherence', 'Fragility', 'BasinCoherence', 'BasinFragility', 'AttractorCoherence', 'AttractorFragility'])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Number of attractors:"}\NormalTok{, results\_exact[}\StringTok{"NumberOfAttractors"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Attractors (decimal states):"}\NormalTok{, results\_exact[}\StringTok{"Attractors"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Eventual attractor:"}\NormalTok{, results\_exact[}\StringTok{"AttractorID"}\NormalTok{])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Basin sizes:"}\NormalTok{, results\_exact[}\StringTok{"BasinSizes"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Overall coherence:"}\NormalTok{, results\_exact[}\StringTok{"Coherence"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Overall fragility:"}\NormalTok{, results\_exact[}\StringTok{"Fragility"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of attractors: 3
Attractors (decimal states): [[0], [2, 5], [7]]
Eventual attractor: [0 1 1 2 1 1 2 2]
Basin sizes: [0.125 0.5   0.375]
Overall coherence: 0.3333333333333333
Overall fragility: 0.3333333333333333
\end{verbatim}

\subsection{Basin-level and attractor-level
robustness}\label{basin-level-and-attractor-level-robustness}

Robustness can be resolved at different structural levels. We now
inspect basin-specific and attractor-specific measures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_basins }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{"BasinSize"}\NormalTok{: results\_exact[}\StringTok{"BasinSizes"}\NormalTok{],}
    \StringTok{"BasinCoherence"}\NormalTok{: results\_exact[}\StringTok{"BasinCoherence"}\NormalTok{],}
    \StringTok{"BasinFragility"}\NormalTok{: results\_exact[}\StringTok{"BasinFragility"}\NormalTok{],}
\NormalTok{\})}

\NormalTok{df\_attractors }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{"AttractorCoherence"}\NormalTok{: results\_exact[}\StringTok{"AttractorCoherence"}\NormalTok{],}
    \StringTok{"AttractorFragility"}\NormalTok{: results\_exact[}\StringTok{"AttractorFragility"}\NormalTok{],}
\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Basin{-}level robustness:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(df\_basins)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Attractor{-}level robustness:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(df\_attractors)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Basin-level robustness:
   BasinSize  BasinCoherence  BasinFragility
0      0.125        0.000000        0.500000
1      0.500        0.333333        0.333333
2      0.375        0.444444        0.277778
Attractor-level robustness:
   AttractorCoherence  AttractorFragility
0            0.000000            0.500000
1            0.333333            0.333333
2            0.666667            0.166667
\end{verbatim}

Interpretation:

\begin{itemize}
\tightlist
\item
  \textbf{Coherence} measures the fraction of single-bit perturbations
  that do \emph{not} change the final attractor.
\item
  \textbf{Fragility} measures how much the attractor state changes
  \emph{when} a perturbation does lead to a different attractor.
\end{itemize}

Importantly, attractors are often less stable than their basins, a
phenomenon explored in detail in Tutorial \#10.

\subsection{Visualization of basin
robustness}\label{visualization-of-basin-robustness}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\NormalTok{ax.bar(}
\NormalTok{    np.arange(}\BuiltInTok{len}\NormalTok{(results\_exact[}\StringTok{"BasinSizes"}\NormalTok{])),}
\NormalTok{    results\_exact[}\StringTok{"BasinFragility"}\NormalTok{],}
\NormalTok{    label}\OperatorTok{=}\StringTok{"Basin fragility"}\NormalTok{,}
\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Basin index"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Fragility"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Exact basin fragility (synchronous update)"}\NormalTok{)}
\NormalTok{ax.set\_ylim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={png}]{tutorial08_stability_analysis_files/tutorial08_stability_analysis_11_0.png}}
\caption{png}
\end{figure}

\subsection{Approximate robustness for larger
networks}\label{approximate-robustness-for-larger-networks}

For larger networks, exact enumeration of all 2\^{}N states is
infeasible. BoolForge therefore provides a Monte Carlo approximation
that samples random initial conditions and perturbations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results\_approx }\OperatorTok{=}\NormalTok{ bn.get\_attractors\_and\_robustness\_synchronous(}
\NormalTok{    n\_simulations}\OperatorTok{=}\DecValTok{500}
\NormalTok{)}

\NormalTok{results\_approx.keys()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
dict_keys(['Attractors', 'LowerBoundOfNumberOfAttractors', 'BasinSizesApproximation', 'CoherenceApproximation', 'FragilityApproximation', 'FinalHammingDistanceApproximation', 'BasinCoherenceApproximation', 'BasinFragilityApproximation', 'AttractorCoherence', 'AttractorFragility'])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Lower bound on number of attractors:"}\NormalTok{, results\_approx[}\StringTok{"LowerBoundOfNumberOfAttractors"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Approximate coherence:"}\NormalTok{, results\_approx[}\StringTok{"CoherenceApproximation"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Approximate fragility:"}\NormalTok{, results\_approx[}\StringTok{"FragilityApproximation"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Final Hamming distance approximation:"}\NormalTok{,}
\NormalTok{      results\_approx[}\StringTok{"FinalHammingDistanceApproximation"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Lower bound on number of attractors: 3
Approximate coherence: 0.322
Approximate fragility: 0.33899999999999997
Final Hamming distance approximation: 0.339
\end{verbatim}

Even for this small network, the approximate values closely match the
exact ones. For larger networks, these approximations are often the only
feasible option.

\subsection{Derrida value: dynamical
sensitivity}\label{derrida-value-dynamical-sensitivity}

The Derrida value measures how perturbations \emph{propagate} after one
synchronous update. It is defined as the expected Hamming distance
between updated states that initially differed in exactly one bit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{derrida\_exact }\OperatorTok{=}\NormalTok{ bn.get\_derrida\_value(exact}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{derrida\_approx }\OperatorTok{=}\NormalTok{ bn.get\_derrida\_value(n\_simulations}\OperatorTok{=}\DecValTok{2000}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Exact Derrida value:"}\NormalTok{, derrida\_exact)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Approximate Derrida value:"}\NormalTok{, derrida\_approx)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Exact Derrida value: 1.0
Approximate Derrida value: 1.0
\end{verbatim}

Interpretation:

\begin{itemize}
\tightlist
\item
  Small Derrida values indicate ordered, stable dynamics.
\item
  Large Derrida values indicate sensitive or chaotic dynamics.
\end{itemize}

Derrida values are closely related to average sensitivity of the update
functions, and provide a complementary notion of robustness to
basin-based measures.

\subsection{Summary and outlook}\label{summary-and-outlook-2}

In this tutorial you learned how to: - compute exact robustness measures
for small Boolean networks, - interpret coherence and fragility at
network, basin, and attractor levels, - approximate robustness measures
for larger networks, and - assess dynamical sensitivity using the
Derrida value.

\textbf{Next steps:} In Tutorial 9, we will move from global robustness
measures to \emph{trajectory-based} sensitivity analysis, including
damage spreading, Hamming distance dynamics, and time-resolved
perturbation experiments.
